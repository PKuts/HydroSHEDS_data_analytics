{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e6d0e42",
   "metadata": {},
   "source": [
    "\n",
    "## Catchments  ## \n",
    "\n",
    "**Pour points**\n",
    "\n",
    "Definition: Specific locations on the river network where we want to measure or model water flow.\n",
    "Examples: Gauging stations, river mouths, dams, hydropower plants.\n",
    "\n",
    "**Catchments**\n",
    " \n",
    "Definition: The area of land where all rainfall drains to the same pour point.\n",
    "Each catchment is linked to exactly one pour point.\n",
    "\n",
    "**Steps**\n",
    "- Clip HydroSHEDS DEM, ACC, DIR Bhutan + buffer.\n",
    "- Create pour points (CSV with lon, lat, id).\n",
    "- Snap pour points to nearest high-ACC pixels (river cells).\n",
    "- Use flow direction (DIR) + snapped points to generate catchments.\n",
    "- Convert catchments to polygons and calculate basic attributes.\n",
    "\n",
    "\n",
    "- Use your clipped DIR (as_dir_Bhutan_and_buffer.tif) to build a catchment ID raster (one ID per basin).\n",
    "- Convert catchments to polygons (optional, for QA/visualization).\n",
    "- Build a point→catchment_id mapping for any Bhutan locations (CSV of lon/lat).\n",
    "- Use the same raster to tag every weather pixel with catchment_id, then group/aggregate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125e47ba",
   "metadata": {},
   "source": [
    "## 1. Crop and Save a Smaller DEM, ACC, DIR (TIF) for Bhutan + Buffer\n",
    "Instead of processing the full Asia-wide TIF files, we first crop and save a smaller GeoTIFF files limited to the Bhutan region and its buffer (latitude 25.0°–29.5°, longitude 87.0°–93.5°). \n",
    "\n",
    "**DEM** — Digital Elevation Model\n",
    "\n",
    "- A raster grid of ground elevation (usually meters above sea level).\n",
    "\n",
    "**DIR** — Flow Direction\n",
    "\n",
    "- A raster showing which neighboring cell water flows to from each cell (downslope).\n",
    "- In HydroSHEDS (ESRI D8), values encode directions: 1=E, 2=SE, 4=S, 8=SW, 16=W, 32=NW, 64=N, 128=NE. \n",
    "- Computed from the DEM.\n",
    "\n",
    "**ACC** — Flow Accumulation\n",
    "\n",
    "- For each cell, how much upstream area drains into it.\n",
    "- High ACC = river channels; used to define streams and to snap pour points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df54dc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Number of bands in TIFF: 1\n",
      "✅ Saved: ../../data/HydroSHEDS/as_dem_Bhutan_and_buffer.tif\n",
      "📐 Size (width x height): 7800.0 x 5400.0\n",
      "💾 File size: 80.37 MB\n",
      "📌 CRS: EPSG:4326\n",
      "🧭 Bounds: BoundingBox(left=87.0, bottom=25.000000000000007, right=93.5, top=29.500000000000007)\n",
      "📦 Data type: int16\n",
      "🧮 NoData value: 32767.0\n",
      "✅ Memory cleaned up.\n"
     ]
    }
   ],
   "source": [
    "#First lets cut DEM\n",
    "import rasterio\n",
    "from rasterio.windows import from_bounds\n",
    "from rasterio.enums import Resampling\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Define the input and output paths\n",
    "input_tif = \"../../data/HydroSHEDS/as_dem_3s.tif\"\n",
    "output_tif = \"../../data/HydroSHEDS/as_dem_Bhutan_and_buffer.tif\"\n",
    "\n",
    "# Define the bounding box for Bhutan + buffer (in degrees)\n",
    "min_lon, max_lon = 87.0, 93.5\n",
    "min_lat, max_lat = 25.0, 29.5\n",
    "\n",
    "# Open the source TIFF file\n",
    "with rasterio.open(input_tif) as src:\n",
    "    print(f\"📦 Number of bands in TIFF: {src.count}\")\n",
    "    if src.count != 1:\n",
    "        raise ValueError(\"❌ Expected only one band in the DEM file.\")\n",
    "\n",
    "    # Compute the pixel window corresponding to the bounding box\n",
    "    window = from_bounds(min_lon, min_lat, max_lon, max_lat, transform=src.transform)\n",
    "\n",
    "    # Read the data within that window (band 1 = elevation)\n",
    "    data = src.read(1, window=window)\n",
    "\n",
    "    # Get the updated transform for the cropped window\n",
    "    transform = src.window_transform(window)\n",
    "\n",
    "    # Save the cropped raster to a new TIF\n",
    "    out_meta = src.meta.copy()\n",
    "    out_meta.update({\n",
    "        \"height\": window.height,\n",
    "        \"width\": window.width,\n",
    "        \"transform\": transform\n",
    "    })\n",
    "\n",
    "    with rasterio.open(output_tif, \"w\", **out_meta) as out_src:\n",
    "        out_src.write(data, 1)\n",
    "\n",
    "print(f\"✅ Saved: {output_tif}\")\n",
    "print(f\"📐 Size (width x height): {window.width} x {window.height}\")\n",
    "\n",
    "# Check file size in MB\n",
    "file_size_mb = os.path.getsize(output_tif) / (1024 * 1024)\n",
    "print(f\"💾 File size: {file_size_mb:.2f} MB\")\n",
    "\n",
    "with rasterio.open(output_tif) as tif_check:\n",
    "    print(f\"📌 CRS: {tif_check.crs}\")\n",
    "    print(f\"🧭 Bounds: {tif_check.bounds}\")\n",
    "    print(f\"📦 Data type: {tif_check.dtypes[0]}\")\n",
    "    print(f\"🧮 NoData value: {tif_check.nodata}\")\n",
    "\n",
    "# 🔥 Clean up memory\n",
    "del data, transform, out_meta, window, tif_check, src, out_src\n",
    "gc.collect()\n",
    "print(\"✅ Memory cleaned up.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4de1e2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Number of bands in TIFF: 1\n",
      "✅ Saved: ../../data/HydroSHEDS/as_acc_Bhutan_and_buffer.tif\n",
      "📐 Size (width x height): 7800.0 x 5400.0\n",
      "💾 File size: 160.71 MB\n",
      "📌 CRS: EPSG:4326\n",
      "🧭 Bounds: BoundingBox(left=87.0, bottom=25.000000000000007, right=93.5, top=29.500000000000007)\n",
      "📦 Data type: uint32\n",
      "🧮 NoData value: 4294967295.0\n",
      "✅ Memory cleaned up (ACC).\n",
      "📦 Number of bands in TIFF: 1\n",
      "✅ Saved: ../../data/HydroSHEDS/as_dir_Bhutan_and_buffer.tif\n",
      "📐 Size (width x height): 7800.0 x 5400.0\n",
      "💾 File size: 40.20 MB\n",
      "📌 CRS: EPSG:4326\n",
      "🧭 Bounds: BoundingBox(left=87.0, bottom=25.000000000000007, right=93.5, top=29.500000000000007)\n",
      "📦 Data type: uint8\n",
      "🧮 NoData value: 255.0\n",
      "✅ Memory cleaned up (DIR).\n"
     ]
    }
   ],
   "source": [
    "#Cut DIR and ACC\n",
    "import rasterio\n",
    "from rasterio.windows import from_bounds\n",
    "from rasterio.enums import Resampling\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# === ACC ===\n",
    "input_tif_acc  = \"../../data/HydroSHEDS/as_acc_3s.tif\"\n",
    "output_tif_acc = \"../../data/HydroSHEDS/as_acc_Bhutan_and_buffer.tif\"\n",
    "\n",
    "# Bhutan + buffer (degrees)\n",
    "min_lon, max_lon = 87.0, 93.5\n",
    "min_lat, max_lat = 25.0, 29.5\n",
    "\n",
    "with rasterio.open(input_tif_acc) as src_acc:\n",
    "    print(f\"📦 Number of bands in TIFF: {src_acc.count}\")\n",
    "    if src_acc.count != 1:\n",
    "        raise ValueError(\"❌ Expected only one band in the ACC file.\")\n",
    "\n",
    "    window_acc = from_bounds(min_lon, min_lat, max_lon, max_lat, transform=src_acc.transform)\n",
    "\n",
    "    data_acc = src_acc.read(1, window=window_acc)\n",
    "    transform_acc = src_acc.window_transform(window_acc)\n",
    "\n",
    "    out_meta_acc = src_acc.meta.copy()\n",
    "    out_meta_acc.update({\n",
    "        \"height\": window_acc.height,\n",
    "        \"width\": window_acc.width,\n",
    "        \"transform\": transform_acc\n",
    "    })\n",
    "\n",
    "    with rasterio.open(output_tif_acc, \"w\", **out_meta_acc) as out_src_acc:\n",
    "        out_src_acc.write(data_acc, 1)\n",
    "\n",
    "print(f\"✅ Saved: {output_tif_acc}\")\n",
    "print(f\"📐 Size (width x height): {window_acc.width} x {window_acc.height}\")\n",
    "\n",
    "file_size_mb_acc = os.path.getsize(output_tif_acc) / (1024 * 1024)\n",
    "print(f\"💾 File size: {file_size_mb_acc:.2f} MB\")\n",
    "\n",
    "with rasterio.open(output_tif_acc) as tif_check_acc:\n",
    "    print(f\"📌 CRS: {tif_check_acc.crs}\")\n",
    "    print(f\"🧭 Bounds: {tif_check_acc.bounds}\")\n",
    "    print(f\"📦 Data type: {tif_check_acc.dtypes[0]}\")\n",
    "    print(f\"🧮 NoData value: {tif_check_acc.nodata}\")\n",
    "\n",
    "# 🔥 Clean up memory (ACC)\n",
    "del data_acc, transform_acc, out_meta_acc, window_acc, tif_check_acc, src_acc, out_src_acc\n",
    "gc.collect()\n",
    "print(\"✅ Memory cleaned up (ACC).\")\n",
    "\n",
    "\n",
    "# === DIR ===\n",
    "input_tif_dir  = \"../../data/HydroSHEDS/as_dir_3s.tif\"\n",
    "output_tif_dir = \"../../data/HydroSHEDS/as_dir_Bhutan_and_buffer.tif\"\n",
    "\n",
    "with rasterio.open(input_tif_dir) as src_dir:\n",
    "    print(f\"📦 Number of bands in TIFF: {src_dir.count}\")\n",
    "    if src_dir.count != 1:\n",
    "        raise ValueError(\"❌ Expected only one band in the DIR file.\")\n",
    "\n",
    "    window_dir = from_bounds(min_lon, min_lat, max_lon, max_lat, transform=src_dir.transform)\n",
    "\n",
    "    data_dir = src_dir.read(1, window=window_dir)\n",
    "    transform_dir = src_dir.window_transform(window_dir)\n",
    "\n",
    "    out_meta_dir = src_dir.meta.copy()\n",
    "    out_meta_dir.update({\n",
    "        \"height\": window_dir.height,\n",
    "        \"width\": window_dir.width,\n",
    "        \"transform\": transform_dir\n",
    "    })\n",
    "\n",
    "    with rasterio.open(output_tif_dir, \"w\", **out_meta_dir) as out_src_dir:\n",
    "        out_src_dir.write(data_dir, 1)\n",
    "\n",
    "print(f\"✅ Saved: {output_tif_dir}\")\n",
    "print(f\"📐 Size (width x height): {window_dir.width} x {window_dir.height}\")\n",
    "\n",
    "file_size_mb_dir = os.path.getsize(output_tif_dir) / (1024 * 1024)\n",
    "print(f\"💾 File size: {file_size_mb_dir:.2f} MB\")\n",
    "\n",
    "with rasterio.open(output_tif_dir) as tif_check_dir:\n",
    "    print(f\"📌 CRS: {tif_check_dir.crs}\")\n",
    "    print(f\"🧭 Bounds: {tif_check_dir.bounds}\")\n",
    "    print(f\"📦 Data type: {tif_check_dir.dtypes[0]}\")\n",
    "    print(f\"🧮 NoData value: {tif_check_dir.nodata}\")\n",
    "\n",
    "# 🔥 Clean up memory (DIR)\n",
    "del data_dir, transform_dir, out_meta_dir, window_dir, tif_check_dir, src_dir, out_src_dir\n",
    "gc.collect()\n",
    "print(\"✅ Memory cleaned up (DIR).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0596a9ea",
   "metadata": {},
   "source": [
    "## 2. Generate catchments (basins) from DIR\n",
    "Note: ESRI-D8 flow direction: a raster layer of flow directions using ESRI’s D8 scheme. Each pixel stores a code for the direction water flows to:\n",
    "1 = E, 2 = SE, 4 = S, 8 = SW, 16 = W, 32 = NW, 64 = N, 128 = NE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ac218ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Installing pyogrio ...\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyogrio\n",
      "  Downloading pyogrio-0.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from pyogrio) (2019.11.28)\n",
      "Requirement already satisfied: numpy in /home/merlin/.local/lib/python3.8/site-packages (from pyogrio) (1.24.4)\n",
      "Requirement already satisfied: packaging in /home/merlin/.local/lib/python3.8/site-packages (from pyogrio) (25.0)\n",
      "Downloading pyogrio-0.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Error parsing dependencies of distro-info: Invalid version: '0.23ubuntu1'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of python-debian: Invalid version: '0.1.36ubuntu1'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: pyogrio\n",
      "Successfully installed pyogrio-0.9.0\n",
      "Using ACC threshold = 10000 cells\n",
      "✅ DIR/ACC aligned: 7800 x 5400 | CRS=EPSG:4326\n",
      "./whitebox_tools --run=\"ExtractStreams\" --wd=\"/home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out\" --flow_accum='/home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/as_acc_Bhutan_and_buffer.tif' --output='/home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/streams.tif' --threshold='10000' -v --compress_rasters=False\n",
      "\n",
      "*****************************\n",
      "* Welcome to ExtractStreams *\n",
      "* Powered by WhiteboxTools  *\n",
      "* www.whiteboxgeo.com       *\n",
      "*****************************\n",
      "Reading data...\n",
      "Progress: 0%\n",
      "Progress: 1%\n",
      "Progress: 2%\n",
      "Progress: 3%\n",
      "Progress: 4%\n",
      "Progress: 5%\n",
      "Progress: 6%\n",
      "Progress: 7%\n",
      "Progress: 8%\n",
      "Progress: 9%\n",
      "Progress: 10%\n",
      "Progress: 11%\n",
      "Progress: 12%\n",
      "Progress: 13%\n",
      "Progress: 14%\n",
      "Progress: 15%\n",
      "Progress: 16%\n",
      "Progress: 17%\n",
      "Progress: 18%\n",
      "Progress: 19%\n",
      "Progress: 20%\n",
      "Progress: 21%\n",
      "Progress: 22%\n",
      "Progress: 23%\n",
      "Progress: 24%\n",
      "Progress: 25%\n",
      "Progress: 26%\n",
      "Progress: 27%\n",
      "Progress: 28%\n",
      "Progress: 29%\n",
      "Progress: 30%\n",
      "Progress: 31%\n",
      "Progress: 32%\n",
      "Progress: 33%\n",
      "Progress: 34%\n",
      "Progress: 35%\n",
      "Progress: 36%\n",
      "Progress: 37%\n",
      "Progress: 38%\n",
      "Progress: 39%\n",
      "Progress: 40%\n",
      "Progress: 41%\n",
      "Progress: 42%\n",
      "Progress: 43%\n",
      "Progress: 44%\n",
      "Progress: 45%\n",
      "Progress: 46%\n",
      "Progress: 47%\n",
      "Progress: 48%\n",
      "Progress: 49%\n",
      "Progress: 50%\n",
      "Progress: 51%\n",
      "Progress: 52%\n",
      "Progress: 53%\n",
      "Progress: 54%\n",
      "Progress: 55%\n",
      "Progress: 56%\n",
      "Progress: 57%\n",
      "Progress: 58%\n",
      "Progress: 59%\n",
      "Progress: 60%\n",
      "Progress: 61%\n",
      "Progress: 62%\n",
      "Progress: 63%\n",
      "Progress: 64%\n",
      "Progress: 65%\n",
      "Progress: 66%\n",
      "Progress: 67%\n",
      "Progress: 68%\n",
      "Progress: 69%\n",
      "Progress: 70%\n",
      "Progress: 71%\n",
      "Progress: 72%\n",
      "Progress: 73%\n",
      "Progress: 74%\n",
      "Progress: 75%\n",
      "Progress: 76%\n",
      "Progress: 77%\n",
      "Progress: 78%\n",
      "Progress: 79%\n",
      "Progress: 80%\n",
      "Progress: 81%\n",
      "Progress: 82%\n",
      "Progress: 83%\n",
      "Progress: 84%\n",
      "Progress: 85%\n",
      "Progress: 86%\n",
      "Progress: 87%\n",
      "Progress: 88%\n",
      "Progress: 89%\n",
      "Progress: 90%\n",
      "Progress: 91%\n",
      "Progress: 92%\n",
      "Progress: 93%\n",
      "Progress: 94%\n",
      "Progress: 95%\n",
      "Progress: 96%\n",
      "Progress: 97%\n",
      "Progress: 98%\n",
      "Progress: 99%\n",
      "Progress: 100%\n",
      "Saving data...\n",
      "Output file written\n",
      "Elapsed Time (excluding I/O): 0.274s\n",
      "✅ Streams: 0 → /home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/streams.tif\n",
      "🔎 Candidate boundary outlets (raw): 8440\n",
      "✅ Clustered outlets (one per mouth): 1761\n",
      "✅ Pour-point raster (clustered): /home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/auto_pour_points_clustered.tif\n",
      "./whitebox_tools --run=\"Watershed\" --wd=\"/home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out\" --d8_pntr='/home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/as_dir_Bhutan_and_buffer.tif' --pour_pts='/home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/auto_pour_points_clustered.tif' --output='/home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/watersheds_by_outlets_clustered.tif' --esri_pntr -v --compress_rasters=False\n",
      "\n",
      "****************************\n",
      "* Welcome to Watershed     *\n",
      "* Powered by WhiteboxTools *\n",
      "* www.whiteboxgeo.com      *\n",
      "****************************\n",
      "Reading data...\n",
      "Initializing: 0%\n",
      "Initializing: 1%\n",
      "Initializing: 2%\n",
      "Initializing: 3%\n",
      "Initializing: 4%\n",
      "Initializing: 5%\n",
      "Initializing: 6%\n",
      "Initializing: 7%\n",
      "Initializing: 8%\n",
      "Initializing: 9%\n",
      "Initializing: 10%\n",
      "Initializing: 11%\n",
      "Initializing: 12%\n",
      "Initializing: 13%\n",
      "Initializing: 14%\n",
      "Initializing: 15%\n",
      "Initializing: 16%\n",
      "Initializing: 17%\n",
      "Initializing: 18%\n",
      "Initializing: 19%\n",
      "Initializing: 20%\n",
      "Initializing: 21%\n",
      "Initializing: 22%\n",
      "Initializing: 23%\n",
      "Initializing: 24%\n",
      "Initializing: 25%\n",
      "Initializing: 26%\n",
      "Initializing: 27%\n",
      "Initializing: 28%\n",
      "Initializing: 29%\n",
      "Initializing: 30%\n",
      "Initializing: 31%\n",
      "Initializing: 32%\n",
      "Initializing: 33%\n",
      "Initializing: 34%\n",
      "Initializing: 35%\n",
      "Initializing: 36%\n",
      "Initializing: 37%\n",
      "Initializing: 38%\n",
      "Initializing: 39%\n",
      "Initializing: 40%\n",
      "Initializing: 41%\n",
      "Initializing: 42%\n",
      "Initializing: 43%\n",
      "Initializing: 44%\n",
      "Initializing: 45%\n",
      "Initializing: 46%\n",
      "Initializing: 47%\n",
      "Initializing: 48%\n",
      "Initializing: 49%\n",
      "Initializing: 50%\n",
      "Initializing: 51%\n",
      "Initializing: 52%\n",
      "Initializing: 53%\n",
      "Initializing: 54%\n",
      "Initializing: 55%\n",
      "Initializing: 56%\n",
      "Initializing: 57%\n",
      "Initializing: 58%\n",
      "Initializing: 59%\n",
      "Initializing: 60%\n",
      "Initializing: 61%\n",
      "Initializing: 62%\n",
      "Initializing: 63%\n",
      "Initializing: 64%\n",
      "Initializing: 65%\n",
      "Initializing: 66%\n",
      "Initializing: 67%\n",
      "Initializing: 68%\n",
      "Initializing: 69%\n",
      "Initializing: 70%\n",
      "Initializing: 71%\n",
      "Initializing: 72%\n",
      "Initializing: 73%\n",
      "Initializing: 74%\n",
      "Initializing: 75%\n",
      "Initializing: 76%\n",
      "Initializing: 77%\n",
      "Initializing: 78%\n",
      "Initializing: 79%\n",
      "Initializing: 80%\n",
      "Initializing: 81%\n",
      "Initializing: 82%\n",
      "Initializing: 83%\n",
      "Initializing: 84%\n",
      "Initializing: 85%\n",
      "Initializing: 86%\n",
      "Initializing: 87%\n",
      "Initializing: 88%\n",
      "Initializing: 89%\n",
      "Initializing: 90%\n",
      "Initializing: 91%\n",
      "Initializing: 92%\n",
      "Initializing: 93%\n",
      "Initializing: 94%\n",
      "Initializing: 95%\n",
      "Initializing: 96%\n",
      "Initializing: 97%\n",
      "Initializing: 98%\n",
      "Initializing: 99%\n",
      "Initializing: 100%\n",
      "Progress: 0%\n",
      "Progress: 1%\n",
      "Progress: 2%\n",
      "Progress: 3%\n",
      "Progress: 4%\n",
      "Progress: 5%\n",
      "Progress: 6%\n",
      "Progress: 7%\n",
      "Progress: 8%\n",
      "Progress: 9%\n",
      "Progress: 10%\n",
      "Progress: 11%\n",
      "Progress: 12%\n",
      "Progress: 13%\n",
      "Progress: 14%\n",
      "Progress: 15%\n",
      "Progress: 16%\n",
      "Progress: 17%\n",
      "Progress: 18%\n",
      "Progress: 19%\n",
      "Progress: 20%\n",
      "Progress: 21%\n",
      "Progress: 22%\n",
      "Progress: 23%\n",
      "Progress: 24%\n",
      "Progress: 25%\n",
      "Progress: 26%\n",
      "Progress: 27%\n",
      "Progress: 28%\n",
      "Progress: 29%\n",
      "Progress: 30%\n",
      "Progress: 31%\n",
      "Progress: 32%\n",
      "Progress: 33%\n",
      "Progress: 34%\n",
      "Progress: 35%\n",
      "Progress: 36%\n",
      "Progress: 37%\n",
      "Progress: 38%\n",
      "Progress: 39%\n",
      "Progress: 40%\n",
      "Progress: 41%\n",
      "Progress: 42%\n",
      "Progress: 43%\n",
      "Progress: 44%\n",
      "Progress: 45%\n",
      "Progress: 46%\n",
      "Progress: 47%\n",
      "Progress: 48%\n",
      "Progress: 49%\n",
      "Progress: 50%\n",
      "Progress: 51%\n",
      "Progress: 52%\n",
      "Progress: 53%\n",
      "Progress: 54%\n",
      "Progress: 55%\n",
      "Progress: 56%\n",
      "Progress: 57%\n",
      "Progress: 58%\n",
      "Progress: 59%\n",
      "Progress: 60%\n",
      "Progress: 61%\n",
      "Progress: 62%\n",
      "Progress: 63%\n",
      "Progress: 64%\n",
      "Progress: 65%\n",
      "Progress: 66%\n",
      "Progress: 67%\n",
      "Progress: 68%\n",
      "Progress: 69%\n",
      "Progress: 70%\n",
      "Progress: 71%\n",
      "Progress: 72%\n",
      "Progress: 73%\n",
      "Progress: 74%\n",
      "Progress: 75%\n",
      "Progress: 76%\n",
      "Progress: 77%\n",
      "Progress: 78%\n",
      "Progress: 79%\n",
      "Progress: 80%\n",
      "Progress: 81%\n",
      "Progress: 82%\n",
      "Progress: 83%\n",
      "Progress: 84%\n",
      "Progress: 85%\n",
      "Progress: 86%\n",
      "Progress: 87%\n",
      "Progress: 88%\n",
      "Progress: 89%\n",
      "Progress: 90%\n",
      "Progress: 91%\n",
      "Progress: 92%\n",
      "Progress: 93%\n",
      "Progress: 94%\n",
      "Progress: 95%\n",
      "Progress: 96%\n",
      "Progress: 97%\n",
      "Progress: 98%\n",
      "Progress: 99%\n",
      "Progress: 100%\n",
      "Saving data...\n",
      "Output file written\n",
      "Elapsed Time (excluding I/O): 2.518s\n",
      "✅ Watersheds by clustered outlets: 0 → /home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/watersheds_by_outlets_clustered.tif\n",
      "./whitebox_tools --run=\"RasterToVectorPolygons\" --wd=\"/home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out\" --input='/home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/watersheds_by_outlets_clustered.tif' --output='/home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/watersheds_by_outlets_clustered.shp' -v --compress_rasters=False\n",
      "\n",
      "*************************************\n",
      "* Welcome to RasterToVectorPolygons *\n",
      "* Powered by WhiteboxTools          *\n",
      "* www.whiteboxgeo.com               *\n",
      "*************************************\n",
      "Reading data...\n",
      "Clumping polygons: 0%\n",
      "Clumping polygons: 1%\n",
      "Clumping polygons: 2%\n",
      "Clumping polygons: 3%\n",
      "Clumping polygons: 4%\n",
      "Clumping polygons: 5%\n",
      "Clumping polygons: 6%\n",
      "Clumping polygons: 7%\n",
      "Clumping polygons: 8%\n",
      "Clumping polygons: 9%\n",
      "Clumping polygons: 10%\n",
      "Clumping polygons: 11%\n",
      "Clumping polygons: 12%\n",
      "Clumping polygons: 13%\n",
      "Clumping polygons: 14%\n",
      "Clumping polygons: 15%\n",
      "Clumping polygons: 16%\n",
      "Clumping polygons: 17%\n",
      "Clumping polygons: 18%\n",
      "Clumping polygons: 19%\n",
      "Clumping polygons: 20%\n",
      "Clumping polygons: 21%\n",
      "Clumping polygons: 22%\n",
      "Clumping polygons: 23%\n",
      "Clumping polygons: 24%\n",
      "Clumping polygons: 25%\n",
      "Clumping polygons: 26%\n",
      "Clumping polygons: 27%\n",
      "Clumping polygons: 28%\n",
      "Clumping polygons: 29%\n",
      "Clumping polygons: 30%\n",
      "Clumping polygons: 31%\n",
      "Clumping polygons: 32%\n",
      "Clumping polygons: 33%\n",
      "Clumping polygons: 34%\n",
      "Clumping polygons: 35%\n",
      "Clumping polygons: 36%\n",
      "Clumping polygons: 37%\n",
      "Clumping polygons: 38%\n",
      "Clumping polygons: 39%\n",
      "Clumping polygons: 40%\n",
      "Clumping polygons: 41%\n",
      "Clumping polygons: 42%\n",
      "Clumping polygons: 43%\n",
      "Clumping polygons: 44%\n",
      "Clumping polygons: 45%\n",
      "Clumping polygons: 46%\n",
      "Clumping polygons: 47%\n",
      "Clumping polygons: 48%\n",
      "Clumping polygons: 49%\n",
      "Clumping polygons: 50%\n",
      "Clumping polygons: 51%\n",
      "Clumping polygons: 52%\n",
      "Clumping polygons: 53%\n",
      "Clumping polygons: 54%\n",
      "Clumping polygons: 55%\n",
      "Clumping polygons: 56%\n",
      "Clumping polygons: 57%\n",
      "Clumping polygons: 58%\n",
      "Clumping polygons: 59%\n",
      "Clumping polygons: 60%\n",
      "Clumping polygons: 61%\n",
      "Clumping polygons: 62%\n",
      "Clumping polygons: 63%\n",
      "Clumping polygons: 64%\n",
      "Clumping polygons: 65%\n",
      "Clumping polygons: 66%\n",
      "Clumping polygons: 67%\n",
      "Clumping polygons: 68%\n",
      "Clumping polygons: 69%\n",
      "Clumping polygons: 70%\n",
      "Clumping polygons: 71%\n",
      "Clumping polygons: 72%\n",
      "Clumping polygons: 73%\n",
      "Clumping polygons: 74%\n",
      "Clumping polygons: 75%\n",
      "Clumping polygons: 76%\n",
      "Clumping polygons: 77%\n",
      "Clumping polygons: 78%\n",
      "Clumping polygons: 79%\n",
      "Clumping polygons: 80%\n",
      "Clumping polygons: 81%\n",
      "Clumping polygons: 82%\n",
      "Clumping polygons: 83%\n",
      "Clumping polygons: 84%\n",
      "Clumping polygons: 85%\n",
      "Clumping polygons: 86%\n",
      "Clumping polygons: 87%\n",
      "Clumping polygons: 88%\n",
      "Clumping polygons: 89%\n",
      "Clumping polygons: 90%\n",
      "Clumping polygons: 91%\n",
      "Clumping polygons: 92%\n",
      "Clumping polygons: 93%\n",
      "Clumping polygons: 94%\n",
      "Clumping polygons: 95%\n",
      "Clumping polygons: 96%\n",
      "Clumping polygons: 97%\n",
      "Clumping polygons: 98%\n",
      "Clumping polygons: 99%\n",
      "Clumping polygons: 100%\n",
      "Finding edges: 0%\n",
      "Finding edges: 1%\n",
      "Finding edges: 2%\n",
      "Finding edges: 3%\n",
      "Finding edges: 4%\n",
      "Finding edges: 5%\n",
      "Finding edges: 6%\n",
      "Finding edges: 7%\n",
      "Finding edges: 8%\n",
      "Finding edges: 9%\n",
      "Finding edges: 10%\n",
      "Finding edges: 11%\n",
      "Finding edges: 12%\n",
      "Finding edges: 13%\n",
      "Finding edges: 14%\n",
      "Finding edges: 15%\n",
      "Finding edges: 16%\n",
      "Finding edges: 17%\n",
      "Finding edges: 18%\n",
      "Finding edges: 19%\n",
      "Finding edges: 20%\n",
      "Finding edges: 21%\n",
      "Finding edges: 22%\n",
      "Finding edges: 23%\n",
      "Finding edges: 24%\n",
      "Finding edges: 25%\n",
      "Finding edges: 26%\n",
      "Finding edges: 27%\n",
      "Finding edges: 28%\n",
      "Finding edges: 29%\n",
      "Finding edges: 30%\n",
      "Finding edges: 31%\n",
      "Finding edges: 32%\n",
      "Finding edges: 33%\n",
      "Finding edges: 34%\n",
      "Finding edges: 35%\n",
      "Finding edges: 36%\n",
      "Finding edges: 37%\n",
      "Finding edges: 38%\n",
      "Finding edges: 39%\n",
      "Finding edges: 40%\n",
      "Finding edges: 41%\n",
      "Finding edges: 42%\n",
      "Finding edges: 43%\n",
      "Finding edges: 44%\n",
      "Finding edges: 45%\n",
      "Finding edges: 46%\n",
      "Finding edges: 47%\n",
      "Finding edges: 48%\n",
      "Finding edges: 49%\n",
      "Finding edges: 50%\n",
      "Finding edges: 51%\n",
      "Finding edges: 52%\n",
      "Finding edges: 53%\n",
      "Finding edges: 54%\n",
      "Finding edges: 55%\n",
      "Finding edges: 56%\n",
      "Finding edges: 57%\n",
      "Finding edges: 58%\n",
      "Finding edges: 59%\n",
      "Finding edges: 60%\n",
      "Finding edges: 61%\n",
      "Finding edges: 62%\n",
      "Finding edges: 63%\n",
      "Finding edges: 64%\n",
      "Finding edges: 65%\n",
      "Finding edges: 66%\n",
      "Finding edges: 67%\n",
      "Finding edges: 68%\n",
      "Finding edges: 69%\n",
      "Finding edges: 70%\n",
      "Finding edges: 71%\n",
      "Finding edges: 72%\n",
      "Finding edges: 73%\n",
      "Finding edges: 74%\n",
      "Finding edges: 75%\n",
      "Finding edges: 76%\n",
      "Finding edges: 77%\n",
      "Finding edges: 78%\n",
      "Finding edges: 79%\n",
      "Finding edges: 80%\n",
      "Finding edges: 81%\n",
      "Finding edges: 82%\n",
      "Finding edges: 83%\n",
      "Finding edges: 84%\n",
      "Finding edges: 85%\n",
      "Finding edges: 86%\n",
      "Finding edges: 87%\n",
      "Finding edges: 88%\n",
      "Finding edges: 89%\n",
      "Finding edges: 90%\n",
      "Finding edges: 91%\n",
      "Finding edges: 92%\n",
      "Finding edges: 93%\n",
      "Finding edges: 94%\n",
      "Finding edges: 95%\n",
      "Finding edges: 96%\n",
      "Finding edges: 97%\n",
      "Finding edges: 98%\n",
      "Finding edges: 99%\n",
      "Finding edges: 100%\n",
      "Tracing polygons: 0%\n",
      "Tracing polygons: 1%\n",
      "Tracing polygons: 2%\n",
      "Tracing polygons: 3%\n",
      "Tracing polygons: 4%\n",
      "Tracing polygons: 5%\n",
      "Tracing polygons: 6%\n",
      "Tracing polygons: 7%\n",
      "Tracing polygons: 8%\n",
      "Tracing polygons: 9%\n",
      "Tracing polygons: 10%\n",
      "Tracing polygons: 11%\n",
      "Tracing polygons: 12%\n",
      "Tracing polygons: 13%\n",
      "Tracing polygons: 14%\n",
      "Tracing polygons: 15%\n",
      "Tracing polygons: 16%\n",
      "Tracing polygons: 17%\n",
      "Tracing polygons: 18%\n",
      "Tracing polygons: 19%\n",
      "Tracing polygons: 20%\n",
      "Tracing polygons: 21%\n",
      "Tracing polygons: 22%\n",
      "Tracing polygons: 23%\n",
      "Tracing polygons: 24%\n",
      "Tracing polygons: 25%\n",
      "Tracing polygons: 26%\n",
      "Tracing polygons: 27%\n",
      "Tracing polygons: 28%\n",
      "Tracing polygons: 29%\n",
      "Tracing polygons: 30%\n",
      "Tracing polygons: 31%\n",
      "Tracing polygons: 32%\n",
      "Tracing polygons: 33%\n",
      "Tracing polygons: 34%\n",
      "Tracing polygons: 35%\n",
      "Tracing polygons: 36%\n",
      "Tracing polygons: 37%\n",
      "Tracing polygons: 38%\n",
      "Tracing polygons: 39%\n",
      "Tracing polygons: 40%\n",
      "Tracing polygons: 41%\n",
      "Tracing polygons: 42%\n",
      "Tracing polygons: 43%\n",
      "Tracing polygons: 44%\n",
      "Tracing polygons: 45%\n",
      "Tracing polygons: 46%\n",
      "Tracing polygons: 47%\n",
      "Tracing polygons: 48%\n",
      "Tracing polygons: 49%\n",
      "Tracing polygons: 50%\n",
      "Tracing polygons: 51%\n",
      "Tracing polygons: 52%\n",
      "Tracing polygons: 53%\n",
      "Tracing polygons: 54%\n",
      "Tracing polygons: 55%\n",
      "Tracing polygons: 56%\n",
      "Tracing polygons: 57%\n",
      "Tracing polygons: 58%\n",
      "Tracing polygons: 59%\n",
      "Tracing polygons: 60%\n",
      "Tracing polygons: 61%\n",
      "Tracing polygons: 62%\n",
      "Tracing polygons: 63%\n",
      "Tracing polygons: 64%\n",
      "Tracing polygons: 65%\n",
      "Tracing polygons: 66%\n",
      "Tracing polygons: 67%\n",
      "Tracing polygons: 68%\n",
      "Tracing polygons: 69%\n",
      "Tracing polygons: 70%\n",
      "Tracing polygons: 71%\n",
      "Tracing polygons: 72%\n",
      "Tracing polygons: 73%\n",
      "Tracing polygons: 74%\n",
      "Tracing polygons: 75%\n",
      "Tracing polygons: 76%\n",
      "Tracing polygons: 77%\n",
      "Tracing polygons: 78%\n",
      "Tracing polygons: 79%\n",
      "Tracing polygons: 80%\n",
      "Tracing polygons: 81%\n",
      "Tracing polygons: 82%\n",
      "Tracing polygons: 83%\n",
      "Tracing polygons: 84%\n",
      "Tracing polygons: 85%\n",
      "Tracing polygons: 86%\n",
      "Tracing polygons: 87%\n",
      "Tracing polygons: 88%\n",
      "Tracing polygons: 89%\n",
      "Tracing polygons: 90%\n",
      "Tracing polygons: 91%\n",
      "Tracing polygons: 92%\n",
      "Tracing polygons: 93%\n",
      "Tracing polygons: 94%\n",
      "Tracing polygons: 95%\n",
      "Tracing polygons: 96%\n",
      "Tracing polygons: 97%\n",
      "Tracing polygons: 98%\n",
      "Tracing polygons: 99%\n",
      "Tracing polygons: 100%\n",
      "Creating geometries: 0%\n",
      "Creating geometries: 1%\n",
      "Creating geometries: 2%\n",
      "Creating geometries: 3%\n",
      "Creating geometries: 4%\n",
      "Creating geometries: 5%\n",
      "Creating geometries: 6%\n",
      "Creating geometries: 7%\n",
      "Creating geometries: 8%\n",
      "Creating geometries: 9%\n",
      "Creating geometries: 10%\n",
      "Creating geometries: 11%\n",
      "Creating geometries: 12%\n",
      "Creating geometries: 13%\n",
      "Creating geometries: 14%\n",
      "Creating geometries: 15%\n",
      "Creating geometries: 16%\n",
      "Creating geometries: 17%\n",
      "Creating geometries: 18%\n",
      "Creating geometries: 19%\n",
      "Creating geometries: 20%\n",
      "Creating geometries: 21%\n",
      "Creating geometries: 22%\n",
      "Creating geometries: 23%\n",
      "Creating geometries: 24%\n",
      "Creating geometries: 25%\n",
      "Creating geometries: 26%\n",
      "Creating geometries: 27%\n",
      "Creating geometries: 28%\n",
      "Creating geometries: 29%\n",
      "Creating geometries: 30%\n",
      "Creating geometries: 31%\n",
      "Creating geometries: 32%\n",
      "Creating geometries: 33%\n",
      "Creating geometries: 34%\n",
      "Creating geometries: 35%\n",
      "Creating geometries: 36%\n",
      "Creating geometries: 37%\n",
      "Creating geometries: 38%\n",
      "Creating geometries: 39%\n",
      "Creating geometries: 40%\n",
      "Creating geometries: 41%\n",
      "Creating geometries: 42%\n",
      "Creating geometries: 43%\n",
      "Creating geometries: 44%\n",
      "Creating geometries: 45%\n",
      "Creating geometries: 46%\n",
      "Creating geometries: 47%\n",
      "Creating geometries: 48%\n",
      "Creating geometries: 49%\n",
      "Creating geometries: 50%\n",
      "Creating geometries: 51%\n",
      "Creating geometries: 52%\n",
      "Creating geometries: 53%\n",
      "Creating geometries: 54%\n",
      "Creating geometries: 55%\n",
      "Creating geometries: 56%\n",
      "Creating geometries: 57%\n",
      "Creating geometries: 58%\n",
      "Creating geometries: 59%\n",
      "Creating geometries: 60%\n",
      "Creating geometries: 61%\n",
      "Creating geometries: 62%\n",
      "Creating geometries: 63%\n",
      "Creating geometries: 64%\n",
      "Creating geometries: 65%\n",
      "Creating geometries: 66%\n",
      "Creating geometries: 67%\n",
      "Creating geometries: 68%\n",
      "Creating geometries: 69%\n",
      "Creating geometries: 70%\n",
      "Creating geometries: 71%\n",
      "Creating geometries: 72%\n",
      "Creating geometries: 73%\n",
      "Creating geometries: 74%\n",
      "Creating geometries: 75%\n",
      "Creating geometries: 76%\n",
      "Creating geometries: 77%\n",
      "Creating geometries: 78%\n",
      "Creating geometries: 79%\n",
      "Creating geometries: 80%\n",
      "Creating geometries: 81%\n",
      "Creating geometries: 82%\n",
      "Creating geometries: 83%\n",
      "Creating geometries: 84%\n",
      "Creating geometries: 85%\n",
      "Creating geometries: 86%\n",
      "Creating geometries: 87%\n",
      "Creating geometries: 88%\n",
      "Creating geometries: 89%\n",
      "Creating geometries: 90%\n",
      "Creating geometries: 91%\n",
      "Creating geometries: 92%\n",
      "Creating geometries: 93%\n",
      "Creating geometries: 94%\n",
      "Creating geometries: 95%\n",
      "Creating geometries: 96%\n",
      "Creating geometries: 97%\n",
      "Creating geometries: 98%\n",
      "Creating geometries: 99%\n",
      "Creating geometries: 100%\n",
      "Saving data...\n",
      "Output file written\n",
      "Elapsed Time (excluding I/O): 13.34s\n",
      "✅ Polygons (SHP): 0 → /home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/watersheds_by_outlets_clustered.shp\n",
      "✅ GeoPackage written (pyogrio): /home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/watersheds_by_outlets_clustered.gpkg\n",
      "Layers: [['watersheds' 'MultiPolygon']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/merlin/.local/lib/python3.8/site-packages/pyogrio/raw.py:196: RuntimeWarning: /home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/watersheds_by_outlets_clustered.shp contains polygon(s) with rings with invalid winding order. Autocorrecting them, but that shapefile should be corrected using ogr2ogr for example.\n",
      "  return ogr_read(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧮 Unique watersheds (clustered): 1761\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# === Install (into THIS kernel) + Build clustered watersheds @ ACC threshold = 10,000 ===\n",
    "# - Installs: pyogrio (for GPKG I/O), geopandas (optional)\n",
    "# - Streams from ACC (threshold = 10,000)\n",
    "# - Boundary outlet candidates -> 8-connected clusters -> pick max-ACC per cluster\n",
    "# - Watershed per selected outlet (ESRI-D8), polygonize to SHP, write GPKG via pyogrio\n",
    "\n",
    "import sys, subprocess, os, gc\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import rasterio\n",
    "\n",
    "# 0) Install libs into THIS kernel (idempotent)\n",
    "def ensure_package(name):\n",
    "    try:\n",
    "        __import__(name)\n",
    "        return\n",
    "    except ImportError:\n",
    "        print(f\"📦 Installing {name} ...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", name])\n",
    "        __import__(name)\n",
    "\n",
    "for pkg in [\"pyogrio\", \"geopandas\"]:\n",
    "    ensure_package(pkg)\n",
    "\n",
    "import pyogrio  # now available\n",
    "\n",
    "# 1) Py3.8 shim so whitebox works on Python < 3.9\n",
    "if sys.version_info < (3, 9):\n",
    "    import importlib.resources as ir\n",
    "    try:\n",
    "        import importlib_resources\n",
    "        if not hasattr(ir, \"files\"):\n",
    "            ir.files = importlib_resources.files\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "import whitebox\n",
    "wbt = whitebox.WhiteboxTools()\n",
    "\n",
    "# 2) Parameters\n",
    "THRESHOLD_CELLS = 10_000  # ACC >= 10k -> stream (~81 km² if 0.0081 km²/pixel)\n",
    "print(f\"Using ACC threshold = {THRESHOLD_CELLS} cells\")\n",
    "\n",
    "# 3) Paths (absolute)\n",
    "root = Path(\"../../data/HydroSHEDS\").resolve()\n",
    "dir_tif = (root / \"as_dir_Bhutan_and_buffer.tif\").resolve()\n",
    "acc_tif = (root / \"as_acc_Bhutan_and_buffer.tif\").resolve()\n",
    "out_dir = (root / \"bt_out\").resolve()\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "streams_tif = out_dir / \"streams.tif\"\n",
    "pp_rast     = out_dir / \"auto_pour_points_clustered.tif\"\n",
    "ws_tif      = out_dir / \"watersheds_by_outlets_clustered.tif\"\n",
    "ws_shp      = out_dir / \"watersheds_by_outlets_clustered.shp\"\n",
    "ws_gpkg     = out_dir / \"watersheds_by_outlets_clustered.gpkg\"\n",
    "\n",
    "def safe_remove(p: Path):\n",
    "    try:\n",
    "        if p.exists():\n",
    "            p.unlink()\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not remove {p}: {e}\")\n",
    "\n",
    "def cleanup_shapefile(stem: Path):\n",
    "    base = stem.with_suffix(\"\")\n",
    "    for ext in (\".shp\", \".shx\", \".dbf\", \".prj\", \".cpg\", \".qmd\"):\n",
    "        safe_remove(base.with_suffix(ext))\n",
    "\n",
    "# clean old outputs\n",
    "for p in [streams_tif, pp_rast, ws_tif, ws_gpkg]:\n",
    "    safe_remove(p)\n",
    "cleanup_shapefile(ws_shp)\n",
    "\n",
    "# 4) Input checks & alignment\n",
    "assert dir_tif.exists(), f\"Missing DIR: {dir_tif}\"\n",
    "assert acc_tif.exists(), f\"Missing ACC: {acc_tif}\"\n",
    "with rasterio.open(dir_tif) as rD, rasterio.open(acc_tif) as rA:\n",
    "    assert rD.crs == rA.crs, \"DIR and ACC CRS differ\"\n",
    "    assert rD.transform == rA.transform, \"DIR and ACC grids not aligned\"\n",
    "    assert (rD.width, rD.height) == (rA.width, rA.height), \"DIR and ACC size mismatch\"\n",
    "    H, W = rD.height, rD.width\n",
    "    profile = rD.profile\n",
    "print(f\"✅ DIR/ACC aligned: {W} x {H} | CRS={profile['crs']}\")\n",
    "\n",
    "wbt.work_dir = str(out_dir)\n",
    "\n",
    "# 5) Streams from ACC\n",
    "ok_s = wbt.extract_streams(flow_accum=str(acc_tif), output=str(streams_tif), threshold=THRESHOLD_CELLS)\n",
    "print(\"✅ Streams:\", ok_s, \"→\", streams_tif)\n",
    "\n",
    "# 6) Boundary outlet candidates (stream on outer border + D8 points outside)\n",
    "with rasterio.open(dir_tif) as r_dir, rasterio.open(streams_tif) as r_str:\n",
    "    dir_arr = r_dir.read(1)\n",
    "    str_arr = r_str.read(1).astype(bool)\n",
    "\n",
    "code2offset = {1:(0,1), 2:(1,1), 4:(1,0), 8:(1,-1), 16:(0,-1), 32:(-1,-1), 64:(-1,0), 128:(-1,1)}\n",
    "\n",
    "cand_coords = []\n",
    "# top/bottom rows\n",
    "for c in range(W):\n",
    "    for r in (0, H-1):\n",
    "        if str_arr[r, c]:\n",
    "            d = int(dir_arr[r, c])\n",
    "            if d in code2offset:\n",
    "                dr, dc = code2offset[d]\n",
    "                nr, nc = r + dr, c + dc\n",
    "                if nr < 0 or nr >= H or nc < 0 or nc >= W:\n",
    "                    cand_coords.append((r, c))\n",
    "# left/right cols (skip corners to avoid dupes)\n",
    "for r in range(1, H-1):\n",
    "    for c in (0, W-1):\n",
    "        if str_arr[r, c]:\n",
    "            d = int(dir_arr[r, c])\n",
    "            if d in code2offset:\n",
    "                dr, dc = code2offset[d]\n",
    "                nr, nc = r + dr, c + dc\n",
    "                if nr < 0 or nr >= H or nc < 0 or nc >= W:\n",
    "                    cand_coords.append((r, c))\n",
    "\n",
    "cand_coords = list(dict.fromkeys(cand_coords))\n",
    "print(f\"🔎 Candidate boundary outlets (raw): {len(cand_coords)}\")\n",
    "\n",
    "# 7) Cluster candidates (8-connectivity) and pick max-ACC per cluster\n",
    "class DSU:\n",
    "    def __init__(self, n):\n",
    "        self.p = list(range(n)); self.r = [0]*n\n",
    "    def find(self, x):\n",
    "        while self.p[x] != x:\n",
    "            self.p[x] = self.p[self.p[x]]\n",
    "            x = self.p[x]\n",
    "        return x\n",
    "    def union(self, a, b):\n",
    "        ra, rb = self.find(a), self.find(b)\n",
    "        if ra == rb: return\n",
    "        if self.r[ra] < self.r[rb]:\n",
    "            self.p[ra] = rb\n",
    "        elif self.r[ra] > self.r[rb]:\n",
    "            self.p[rb] = ra\n",
    "        else:\n",
    "            self.p[rb] = ra; self.r[ra] += 1\n",
    "\n",
    "N = len(cand_coords)\n",
    "idx_map = {rc:i for i, rc in enumerate(cand_coords)}\n",
    "cand_set = set(cand_coords)\n",
    "dsu = DSU(N)\n",
    "nbrs = [(dr, dc) for dr in (-1,0,1) for dc in (-1,0,1) if not (dr==0 and dc==0)]\n",
    "\n",
    "for i, (r, c) in enumerate(cand_coords):\n",
    "    for dr, dc in nbrs:\n",
    "        nr, nc = r+dr, c+dc\n",
    "        if (nr, nc) in cand_set:\n",
    "            dsu.union(i, idx_map[(nr, nc)])\n",
    "\n",
    "groups = {}\n",
    "for i in range(N):\n",
    "    root_i = dsu.find(i)\n",
    "    groups.setdefault(root_i, []).append(i)\n",
    "\n",
    "with rasterio.open(acc_tif) as r_acc:\n",
    "    acc = r_acc.read(1)\n",
    "    acc_nodata = r_acc.nodata\n",
    "\n",
    "selected_rc = []\n",
    "for root_i, members in groups.items():\n",
    "    best_rc, best_val = None, -1\n",
    "    for i in members:\n",
    "        rr, cc = cand_coords[i]\n",
    "        val = acc[rr, cc]\n",
    "        if acc_nodata is not None and val == acc_nodata:\n",
    "            continue\n",
    "        if val > best_val:\n",
    "            best_val = val; best_rc = (rr, cc)\n",
    "    if best_rc is None:\n",
    "        best_rc = cand_coords[members[0]]\n",
    "    selected_rc.append(best_rc)\n",
    "\n",
    "print(f\"✅ Clustered outlets (one per mouth): {len(selected_rc)}\")\n",
    "\n",
    "if not selected_rc:\n",
    "    raise RuntimeError(\"No clustered outlets found. Increase threshold or verify DIR/ACC.\")\n",
    "\n",
    "# 8) Rasterize pour-points (unique IDs)\n",
    "pp_arr = np.zeros((H, W), dtype=np.int32)\n",
    "for i, (rr, cc) in enumerate(selected_rc, start=1):\n",
    "    pp_arr[rr, cc] = i\n",
    "\n",
    "profile_pp = profile.copy()\n",
    "profile_pp.update(dtype=rasterio.int32, count=1, compress=\"deflate\", tiled=True, BIGTIFF=\"IF_SAFER\")\n",
    "with rasterio.open(pp_rast, \"w\", **profile_pp) as dst:\n",
    "    dst.write(pp_arr, 1)\n",
    "print(\"✅ Pour-point raster (clustered):\", pp_rast)\n",
    "\n",
    "# 9) Watershed per clustered outlet (ESRI D8)\n",
    "ok_w = wbt.watershed(d8_pntr=str(dir_tif), pour_pts=str(pp_rast), output=str(ws_tif), esri_pntr=True)\n",
    "print(\"✅ Watersheds by clustered outlets:\", ok_w, \"→\", ws_tif)\n",
    "\n",
    "# 10) Polygonize to SHP (Whitebox), then write GPKG via pyogrio (no Fiona)\n",
    "ok_p = wbt.raster_to_vector_polygons(i=str(ws_tif), output=str(ws_shp))\n",
    "print(\"✅ Polygons (SHP):\", ok_p, \"→\", ws_shp)\n",
    "\n",
    "gdf = pyogrio.read_dataframe(str(ws_shp))\n",
    "if gdf.crs is None:\n",
    "    with rasterio.open(ws_tif) as rt:\n",
    "        gdf.set_crs(rt.crs, inplace=True)\n",
    "pyogrio.write_dataframe(\n",
    "    gdf,\n",
    "    str(ws_gpkg),\n",
    "    layer=\"watersheds\",\n",
    "    driver=\"GPKG\",\n",
    "    append=False\n",
    ")\n",
    "print(\"✅ GeoPackage written (pyogrio):\", ws_gpkg)\n",
    "try:\n",
    "    print(\"Layers:\", pyogrio.list_layers(str(ws_gpkg)))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# 11) Count unique watersheds\n",
    "with rasterio.open(ws_tif) as r:\n",
    "    WZ = r.read(1)\n",
    "    nd = r.nodata\n",
    "    n_ws = np.unique(WZ[WZ != nd]).size\n",
    "print(\"🧮 Unique watersheds (clustered):\", n_ws)\n",
    "\n",
    "gc.collect()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a661fc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DIR/ACC aligned: 7800x5400 | CRS=EPSG:4326\n",
      "./whitebox_tools --run=\"ExtractStreams\" --wd=\"/home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out\" --flow_accum='/home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/as_acc_Bhutan_and_buffer.tif' --output='/home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/streams.tif' --threshold='10000' -v --compress_rasters=False\n",
      "\n",
      "*****************************\n",
      "* Welcome to ExtractStreams *\n",
      "* Powered by WhiteboxTools  *\n",
      "* www.whiteboxgeo.com       *\n",
      "*****************************\n",
      "Reading data...\n",
      "Progress: 0%\n",
      "Progress: 1%\n",
      "Progress: 2%\n",
      "Progress: 3%\n",
      "Progress: 4%\n",
      "Progress: 5%\n",
      "Progress: 6%\n",
      "Progress: 7%\n",
      "Progress: 8%\n",
      "Progress: 9%\n",
      "Progress: 10%\n",
      "Progress: 11%\n",
      "Progress: 12%\n",
      "Progress: 13%\n",
      "Progress: 14%\n",
      "Progress: 15%\n",
      "Progress: 16%\n",
      "Progress: 17%\n",
      "Progress: 18%\n",
      "Progress: 19%\n",
      "Progress: 20%\n",
      "Progress: 21%\n",
      "Progress: 22%\n",
      "Progress: 23%\n",
      "Progress: 24%\n",
      "Progress: 25%\n",
      "Progress: 26%\n",
      "Progress: 27%\n",
      "Progress: 28%\n",
      "Progress: 29%\n",
      "Progress: 30%\n",
      "Progress: 31%\n",
      "Progress: 32%\n",
      "Progress: 33%\n",
      "Progress: 34%\n",
      "Progress: 35%\n",
      "Progress: 36%\n",
      "Progress: 37%\n",
      "Progress: 38%\n",
      "Progress: 39%\n",
      "Progress: 40%\n",
      "Progress: 41%\n",
      "Progress: 42%\n",
      "Progress: 43%\n",
      "Progress: 44%\n",
      "Progress: 45%\n",
      "Progress: 46%\n",
      "Progress: 47%\n",
      "Progress: 48%\n",
      "Progress: 49%\n",
      "Progress: 50%\n",
      "Progress: 51%\n",
      "Progress: 52%\n",
      "Progress: 53%\n",
      "Progress: 54%\n",
      "Progress: 55%\n",
      "Progress: 56%\n",
      "Progress: 57%\n",
      "Progress: 58%\n",
      "Progress: 59%\n",
      "Progress: 60%\n",
      "Progress: 61%\n",
      "Progress: 62%\n",
      "Progress: 63%\n",
      "Progress: 64%\n",
      "Progress: 65%\n",
      "Progress: 66%\n",
      "Progress: 67%\n",
      "Progress: 68%\n",
      "Progress: 69%\n",
      "Progress: 70%\n",
      "Progress: 71%\n",
      "Progress: 72%\n",
      "Progress: 73%\n",
      "Progress: 74%\n",
      "Progress: 75%\n",
      "Progress: 76%\n",
      "Progress: 77%\n",
      "Progress: 78%\n",
      "Progress: 79%\n",
      "Progress: 80%\n",
      "Progress: 81%\n",
      "Progress: 82%\n",
      "Progress: 83%\n",
      "Progress: 84%\n",
      "Progress: 85%\n",
      "Progress: 86%\n",
      "Progress: 87%\n",
      "Progress: 88%\n",
      "Progress: 89%\n",
      "Progress: 90%\n",
      "Progress: 91%\n",
      "Progress: 92%\n",
      "Progress: 93%\n",
      "Progress: 94%\n",
      "Progress: 95%\n",
      "Progress: 96%\n",
      "Progress: 97%\n",
      "Progress: 98%\n",
      "Progress: 99%\n",
      "Progress: 100%\n",
      "Saving data...\n",
      "Output file written\n",
      "Elapsed Time (excluding I/O): 0.334s\n",
      "✅ Streams: 0 → /home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/streams.tif\n",
      "🔎 Boundary outlet candidates (raw): 8440\n",
      "✅ After ACC filter (≥ 20000 cells): 47\n",
      "✅ After proximity merge (≤ 4px): 47\n",
      "✅ Pour-point raster: /home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/auto_pour_points_filtered_merged.tif\n",
      "./whitebox_tools --run=\"Watershed\" --wd=\"/home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out\" --d8_pntr='/home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/as_dir_Bhutan_and_buffer.tif' --pour_pts='/home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/auto_pour_points_filtered_merged.tif' --output='/home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/watersheds_filtered_merged.tif' --esri_pntr -v --compress_rasters=False\n",
      "\n",
      "****************************\n",
      "* Welcome to Watershed     *\n",
      "* Powered by WhiteboxTools *\n",
      "* www.whiteboxgeo.com      *\n",
      "****************************\n",
      "Reading data...\n",
      "Initializing: 0%\n",
      "Initializing: 1%\n",
      "Initializing: 2%\n",
      "Initializing: 3%\n",
      "Initializing: 4%\n",
      "Initializing: 5%\n",
      "Initializing: 6%\n",
      "Initializing: 7%\n",
      "Initializing: 8%\n",
      "Initializing: 9%\n",
      "Initializing: 10%\n",
      "Initializing: 11%\n",
      "Initializing: 12%\n",
      "Initializing: 13%\n",
      "Initializing: 14%\n",
      "Initializing: 15%\n",
      "Initializing: 16%\n",
      "Initializing: 17%\n",
      "Initializing: 18%\n",
      "Initializing: 19%\n",
      "Initializing: 20%\n",
      "Initializing: 21%\n",
      "Initializing: 22%\n",
      "Initializing: 23%\n",
      "Initializing: 24%\n",
      "Initializing: 25%\n",
      "Initializing: 26%\n",
      "Initializing: 27%\n",
      "Initializing: 28%\n",
      "Initializing: 29%\n",
      "Initializing: 30%\n",
      "Initializing: 31%\n",
      "Initializing: 32%\n",
      "Initializing: 33%\n",
      "Initializing: 34%\n",
      "Initializing: 35%\n",
      "Initializing: 36%\n",
      "Initializing: 37%\n",
      "Initializing: 38%\n",
      "Initializing: 39%\n",
      "Initializing: 40%\n",
      "Initializing: 41%\n",
      "Initializing: 42%\n",
      "Initializing: 43%\n",
      "Initializing: 44%\n",
      "Initializing: 45%\n",
      "Initializing: 46%\n",
      "Initializing: 47%\n",
      "Initializing: 48%\n",
      "Initializing: 49%\n",
      "Initializing: 50%\n",
      "Initializing: 51%\n",
      "Initializing: 52%\n",
      "Initializing: 53%\n",
      "Initializing: 54%\n",
      "Initializing: 55%\n",
      "Initializing: 56%\n",
      "Initializing: 57%\n",
      "Initializing: 58%\n",
      "Initializing: 59%\n",
      "Initializing: 60%\n",
      "Initializing: 61%\n",
      "Initializing: 62%\n",
      "Initializing: 63%\n",
      "Initializing: 64%\n",
      "Initializing: 65%\n",
      "Initializing: 66%\n",
      "Initializing: 67%\n",
      "Initializing: 68%\n",
      "Initializing: 69%\n",
      "Initializing: 70%\n",
      "Initializing: 71%\n",
      "Initializing: 72%\n",
      "Initializing: 73%\n",
      "Initializing: 74%\n",
      "Initializing: 75%\n",
      "Initializing: 76%\n",
      "Initializing: 77%\n",
      "Initializing: 78%\n",
      "Initializing: 79%\n",
      "Initializing: 80%\n",
      "Initializing: 81%\n",
      "Initializing: 82%\n",
      "Initializing: 83%\n",
      "Initializing: 84%\n",
      "Initializing: 85%\n",
      "Initializing: 86%\n",
      "Initializing: 87%\n",
      "Initializing: 88%\n",
      "Initializing: 89%\n",
      "Initializing: 90%\n",
      "Initializing: 91%\n",
      "Initializing: 92%\n",
      "Initializing: 93%\n",
      "Initializing: 94%\n",
      "Initializing: 95%\n",
      "Initializing: 96%\n",
      "Initializing: 97%\n",
      "Initializing: 98%\n",
      "Initializing: 99%\n",
      "Initializing: 100%\n",
      "Progress: 0%\n",
      "Progress: 1%\n",
      "Progress: 2%\n",
      "Progress: 3%\n",
      "Progress: 4%\n",
      "Progress: 5%\n",
      "Progress: 6%\n",
      "Progress: 7%\n",
      "Progress: 8%\n",
      "Progress: 9%\n",
      "Progress: 10%\n",
      "Progress: 11%\n",
      "Progress: 12%\n",
      "Progress: 13%\n",
      "Progress: 14%\n",
      "Progress: 15%\n",
      "Progress: 16%\n",
      "Progress: 17%\n",
      "Progress: 18%\n",
      "Progress: 19%\n",
      "Progress: 20%\n",
      "Progress: 21%\n",
      "Progress: 22%\n",
      "Progress: 23%\n",
      "Progress: 24%\n",
      "Progress: 25%\n",
      "Progress: 26%\n",
      "Progress: 27%\n",
      "Progress: 28%\n",
      "Progress: 29%\n",
      "Progress: 30%\n",
      "Progress: 31%\n",
      "Progress: 32%\n",
      "Progress: 33%\n",
      "Progress: 34%\n",
      "Progress: 35%\n",
      "Progress: 36%\n",
      "Progress: 37%\n",
      "Progress: 38%\n",
      "Progress: 39%\n",
      "Progress: 40%\n",
      "Progress: 41%\n",
      "Progress: 42%\n",
      "Progress: 43%\n",
      "Progress: 44%\n",
      "Progress: 45%\n",
      "Progress: 46%\n",
      "Progress: 47%\n",
      "Progress: 48%\n",
      "Progress: 49%\n",
      "Progress: 50%\n",
      "Progress: 51%\n",
      "Progress: 52%\n",
      "Progress: 53%\n",
      "Progress: 54%\n",
      "Progress: 55%\n",
      "Progress: 56%\n",
      "Progress: 57%\n",
      "Progress: 58%\n",
      "Progress: 59%\n",
      "Progress: 60%\n",
      "Progress: 61%\n",
      "Progress: 62%\n",
      "Progress: 63%\n",
      "Progress: 64%\n",
      "Progress: 65%\n",
      "Progress: 66%\n",
      "Progress: 67%\n",
      "Progress: 68%\n",
      "Progress: 69%\n",
      "Progress: 70%\n",
      "Progress: 71%\n",
      "Progress: 72%\n",
      "Progress: 73%\n",
      "Progress: 74%\n",
      "Progress: 75%\n",
      "Progress: 76%\n",
      "Progress: 77%\n",
      "Progress: 78%\n",
      "Progress: 79%\n",
      "Progress: 80%\n",
      "Progress: 81%\n",
      "Progress: 82%\n",
      "Progress: 83%\n",
      "Progress: 84%\n",
      "Progress: 85%\n",
      "Progress: 86%\n",
      "Progress: 87%\n",
      "Progress: 88%\n",
      "Progress: 89%\n",
      "Progress: 90%\n",
      "Progress: 91%\n",
      "Progress: 92%\n",
      "Progress: 93%\n",
      "Progress: 94%\n",
      "Progress: 95%\n",
      "Progress: 96%\n",
      "Progress: 97%\n",
      "Progress: 98%\n",
      "Progress: 99%\n",
      "Progress: 100%\n",
      "Saving data...\n",
      "Output file written\n",
      "Elapsed Time (excluding I/O): 2.356s\n",
      "✅ Watersheds: 0 → /home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/watersheds_filtered_merged.tif\n",
      "./whitebox_tools --run=\"RasterToVectorPolygons\" --wd=\"/home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out\" --input='/home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/watersheds_filtered_merged.tif' --output='/home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/watersheds_filtered_merged.shp' -v --compress_rasters=False\n",
      "\n",
      "*************************************\n",
      "* Welcome to RasterToVectorPolygons *\n",
      "* Powered by WhiteboxTools          *\n",
      "* www.whiteboxgeo.com               *\n",
      "*************************************\n",
      "Reading data...\n",
      "Clumping polygons: 0%\n",
      "Clumping polygons: 1%\n",
      "Clumping polygons: 2%\n",
      "Clumping polygons: 3%\n",
      "Clumping polygons: 4%\n",
      "Clumping polygons: 5%\n",
      "Clumping polygons: 6%\n",
      "Clumping polygons: 7%\n",
      "Clumping polygons: 8%\n",
      "Clumping polygons: 9%\n",
      "Clumping polygons: 10%\n",
      "Clumping polygons: 11%\n",
      "Clumping polygons: 12%\n",
      "Clumping polygons: 13%\n",
      "Clumping polygons: 14%\n",
      "Clumping polygons: 15%\n",
      "Clumping polygons: 16%\n",
      "Clumping polygons: 17%\n",
      "Clumping polygons: 18%\n",
      "Clumping polygons: 19%\n",
      "Clumping polygons: 20%\n",
      "Clumping polygons: 21%\n",
      "Clumping polygons: 22%\n",
      "Clumping polygons: 23%\n",
      "Clumping polygons: 24%\n",
      "Clumping polygons: 25%\n",
      "Clumping polygons: 26%\n",
      "Clumping polygons: 27%\n",
      "Clumping polygons: 28%\n",
      "Clumping polygons: 29%\n",
      "Clumping polygons: 30%\n",
      "Clumping polygons: 31%\n",
      "Clumping polygons: 32%\n",
      "Clumping polygons: 33%\n",
      "Clumping polygons: 34%\n",
      "Clumping polygons: 35%\n",
      "Clumping polygons: 36%\n",
      "Clumping polygons: 37%\n",
      "Clumping polygons: 38%\n",
      "Clumping polygons: 39%\n",
      "Clumping polygons: 40%\n",
      "Clumping polygons: 41%\n",
      "Clumping polygons: 42%\n",
      "Clumping polygons: 43%\n",
      "Clumping polygons: 44%\n",
      "Clumping polygons: 45%\n",
      "Clumping polygons: 46%\n",
      "Clumping polygons: 47%\n",
      "Clumping polygons: 48%\n",
      "Clumping polygons: 49%\n",
      "Clumping polygons: 50%\n",
      "Clumping polygons: 51%\n",
      "Clumping polygons: 52%\n",
      "Clumping polygons: 53%\n",
      "Clumping polygons: 54%\n",
      "Clumping polygons: 55%\n",
      "Clumping polygons: 56%\n",
      "Clumping polygons: 57%\n",
      "Clumping polygons: 58%\n",
      "Clumping polygons: 59%\n",
      "Clumping polygons: 60%\n",
      "Clumping polygons: 61%\n",
      "Clumping polygons: 62%\n",
      "Clumping polygons: 63%\n",
      "Clumping polygons: 64%\n",
      "Clumping polygons: 65%\n",
      "Clumping polygons: 66%\n",
      "Clumping polygons: 67%\n",
      "Clumping polygons: 68%\n",
      "Clumping polygons: 69%\n",
      "Clumping polygons: 70%\n",
      "Clumping polygons: 71%\n",
      "Clumping polygons: 72%\n",
      "Clumping polygons: 73%\n",
      "Clumping polygons: 74%\n",
      "Clumping polygons: 75%\n",
      "Clumping polygons: 76%\n",
      "Clumping polygons: 77%\n",
      "Clumping polygons: 78%\n",
      "Clumping polygons: 79%\n",
      "Clumping polygons: 80%\n",
      "Clumping polygons: 81%\n",
      "Clumping polygons: 82%\n",
      "Clumping polygons: 83%\n",
      "Clumping polygons: 84%\n",
      "Clumping polygons: 85%\n",
      "Clumping polygons: 86%\n",
      "Clumping polygons: 87%\n",
      "Clumping polygons: 88%\n",
      "Clumping polygons: 89%\n",
      "Clumping polygons: 90%\n",
      "Clumping polygons: 91%\n",
      "Clumping polygons: 92%\n",
      "Clumping polygons: 93%\n",
      "Clumping polygons: 94%\n",
      "Clumping polygons: 95%\n",
      "Clumping polygons: 96%\n",
      "Clumping polygons: 97%\n",
      "Clumping polygons: 98%\n",
      "Clumping polygons: 99%\n",
      "Clumping polygons: 100%\n",
      "Finding edges: 0%\n",
      "Finding edges: 1%\n",
      "Finding edges: 2%\n",
      "Finding edges: 3%\n",
      "Finding edges: 4%\n",
      "Finding edges: 5%\n",
      "Finding edges: 6%\n",
      "Finding edges: 7%\n",
      "Finding edges: 8%\n",
      "Finding edges: 9%\n",
      "Finding edges: 10%\n",
      "Finding edges: 11%\n",
      "Finding edges: 12%\n",
      "Finding edges: 13%\n",
      "Finding edges: 14%\n",
      "Finding edges: 15%\n",
      "Finding edges: 16%\n",
      "Finding edges: 17%\n",
      "Finding edges: 18%\n",
      "Finding edges: 19%\n",
      "Finding edges: 20%\n",
      "Finding edges: 21%\n",
      "Finding edges: 22%\n",
      "Finding edges: 23%\n",
      "Finding edges: 24%\n",
      "Finding edges: 25%\n",
      "Finding edges: 26%\n",
      "Finding edges: 27%\n",
      "Finding edges: 28%\n",
      "Finding edges: 29%\n",
      "Finding edges: 30%\n",
      "Finding edges: 31%\n",
      "Finding edges: 32%\n",
      "Finding edges: 33%\n",
      "Finding edges: 34%\n",
      "Finding edges: 35%\n",
      "Finding edges: 36%\n",
      "Finding edges: 37%\n",
      "Finding edges: 38%\n",
      "Finding edges: 39%\n",
      "Finding edges: 40%\n",
      "Finding edges: 41%\n",
      "Finding edges: 42%\n",
      "Finding edges: 43%\n",
      "Finding edges: 44%\n",
      "Finding edges: 45%\n",
      "Finding edges: 46%\n",
      "Finding edges: 47%\n",
      "Finding edges: 48%\n",
      "Finding edges: 49%\n",
      "Finding edges: 50%\n",
      "Finding edges: 51%\n",
      "Finding edges: 52%\n",
      "Finding edges: 53%\n",
      "Finding edges: 54%\n",
      "Finding edges: 55%\n",
      "Finding edges: 56%\n",
      "Finding edges: 57%\n",
      "Finding edges: 58%\n",
      "Finding edges: 59%\n",
      "Finding edges: 60%\n",
      "Finding edges: 61%\n",
      "Finding edges: 62%\n",
      "Finding edges: 63%\n",
      "Finding edges: 64%\n",
      "Finding edges: 65%\n",
      "Finding edges: 66%\n",
      "Finding edges: 67%\n",
      "Finding edges: 68%\n",
      "Finding edges: 69%\n",
      "Finding edges: 70%\n",
      "Finding edges: 71%\n",
      "Finding edges: 72%\n",
      "Finding edges: 73%\n",
      "Finding edges: 74%\n",
      "Finding edges: 75%\n",
      "Finding edges: 76%\n",
      "Finding edges: 77%\n",
      "Finding edges: 78%\n",
      "Finding edges: 79%\n",
      "Finding edges: 80%\n",
      "Finding edges: 81%\n",
      "Finding edges: 82%\n",
      "Finding edges: 83%\n",
      "Finding edges: 84%\n",
      "Finding edges: 85%\n",
      "Finding edges: 86%\n",
      "Finding edges: 87%\n",
      "Finding edges: 88%\n",
      "Finding edges: 89%\n",
      "Finding edges: 90%\n",
      "Finding edges: 91%\n",
      "Finding edges: 92%\n",
      "Finding edges: 93%\n",
      "Finding edges: 94%\n",
      "Finding edges: 95%\n",
      "Finding edges: 96%\n",
      "Finding edges: 97%\n",
      "Finding edges: 98%\n",
      "Finding edges: 99%\n",
      "Finding edges: 100%\n",
      "Tracing polygons: 0%\n",
      "Tracing polygons: 1%\n",
      "Tracing polygons: 2%\n",
      "Tracing polygons: 3%\n",
      "Tracing polygons: 4%\n",
      "Tracing polygons: 5%\n",
      "Tracing polygons: 6%\n",
      "Tracing polygons: 7%\n",
      "Tracing polygons: 8%\n",
      "Tracing polygons: 9%\n",
      "Tracing polygons: 10%\n",
      "Tracing polygons: 11%\n",
      "Tracing polygons: 12%\n",
      "Tracing polygons: 13%\n",
      "Tracing polygons: 14%\n",
      "Tracing polygons: 15%\n",
      "Tracing polygons: 16%\n",
      "Tracing polygons: 17%\n",
      "Tracing polygons: 18%\n",
      "Tracing polygons: 19%\n",
      "Tracing polygons: 20%\n",
      "Tracing polygons: 21%\n",
      "Tracing polygons: 22%\n",
      "Tracing polygons: 23%\n",
      "Tracing polygons: 24%\n",
      "Tracing polygons: 25%\n",
      "Tracing polygons: 26%\n",
      "Tracing polygons: 27%\n",
      "Tracing polygons: 28%\n",
      "Tracing polygons: 29%\n",
      "Tracing polygons: 30%\n",
      "Tracing polygons: 31%\n",
      "Tracing polygons: 32%\n",
      "Tracing polygons: 33%\n",
      "Tracing polygons: 34%\n",
      "Tracing polygons: 35%\n",
      "Tracing polygons: 36%\n",
      "Tracing polygons: 37%\n",
      "Tracing polygons: 38%\n",
      "Tracing polygons: 39%\n",
      "Tracing polygons: 40%\n",
      "Tracing polygons: 41%\n",
      "Tracing polygons: 42%\n",
      "Tracing polygons: 43%\n",
      "Tracing polygons: 44%\n",
      "Tracing polygons: 45%\n",
      "Tracing polygons: 46%\n",
      "Tracing polygons: 47%\n",
      "Tracing polygons: 48%\n",
      "Tracing polygons: 49%\n",
      "Tracing polygons: 50%\n",
      "Tracing polygons: 51%\n",
      "Tracing polygons: 52%\n",
      "Tracing polygons: 53%\n",
      "Tracing polygons: 54%\n",
      "Tracing polygons: 55%\n",
      "Tracing polygons: 56%\n",
      "Tracing polygons: 57%\n",
      "Tracing polygons: 58%\n",
      "Tracing polygons: 59%\n",
      "Tracing polygons: 60%\n",
      "Tracing polygons: 61%\n",
      "Tracing polygons: 62%\n",
      "Tracing polygons: 63%\n",
      "Tracing polygons: 64%\n",
      "Tracing polygons: 65%\n",
      "Tracing polygons: 66%\n",
      "Tracing polygons: 67%\n",
      "Tracing polygons: 68%\n",
      "Tracing polygons: 69%\n",
      "Tracing polygons: 70%\n",
      "Tracing polygons: 71%\n",
      "Tracing polygons: 72%\n",
      "Tracing polygons: 73%\n",
      "Tracing polygons: 74%\n",
      "Tracing polygons: 75%\n",
      "Tracing polygons: 76%\n",
      "Tracing polygons: 77%\n",
      "Tracing polygons: 78%\n",
      "Tracing polygons: 79%\n",
      "Tracing polygons: 80%\n",
      "Tracing polygons: 81%\n",
      "Tracing polygons: 82%\n",
      "Tracing polygons: 83%\n",
      "Tracing polygons: 84%\n",
      "Tracing polygons: 85%\n",
      "Tracing polygons: 86%\n",
      "Tracing polygons: 87%\n",
      "Tracing polygons: 88%\n",
      "Tracing polygons: 89%\n",
      "Tracing polygons: 90%\n",
      "Tracing polygons: 91%\n",
      "Tracing polygons: 92%\n",
      "Tracing polygons: 93%\n",
      "Tracing polygons: 94%\n",
      "Tracing polygons: 95%\n",
      "Tracing polygons: 96%\n",
      "Tracing polygons: 97%\n",
      "Tracing polygons: 98%\n",
      "Tracing polygons: 99%\n",
      "Tracing polygons: 100%\n",
      "Creating geometries: 0%\n",
      "Creating geometries: 2%\n",
      "Creating geometries: 4%\n",
      "Creating geometries: 6%\n",
      "Creating geometries: 8%\n",
      "Creating geometries: 10%\n",
      "Creating geometries: 13%\n",
      "Creating geometries: 15%\n",
      "Creating geometries: 17%\n",
      "Creating geometries: 19%\n",
      "Creating geometries: 21%\n",
      "Creating geometries: 23%\n",
      "Creating geometries: 26%\n",
      "Creating geometries: 28%\n",
      "Creating geometries: 30%\n",
      "Creating geometries: 32%\n",
      "Creating geometries: 34%\n",
      "Creating geometries: 36%\n",
      "Creating geometries: 39%\n",
      "Creating geometries: 41%\n",
      "Creating geometries: 43%\n",
      "Creating geometries: 45%\n",
      "Creating geometries: 47%\n",
      "Creating geometries: 50%\n",
      "Creating geometries: 52%\n",
      "Creating geometries: 54%\n",
      "Creating geometries: 56%\n",
      "Creating geometries: 58%\n",
      "Creating geometries: 60%\n",
      "Creating geometries: 63%\n",
      "Creating geometries: 65%\n",
      "Creating geometries: 67%\n",
      "Creating geometries: 69%\n",
      "Creating geometries: 71%\n",
      "Creating geometries: 73%\n",
      "Creating geometries: 76%\n",
      "Creating geometries: 78%\n",
      "Creating geometries: 80%\n",
      "Creating geometries: 82%\n",
      "Creating geometries: 84%\n",
      "Creating geometries: 86%\n",
      "Creating geometries: 89%\n",
      "Creating geometries: 91%\n",
      "Creating geometries: 93%\n",
      "Creating geometries: 95%\n",
      "Creating geometries: 97%\n",
      "Creating geometries: 100%\n",
      "Saving data...\n",
      "Output file written\n",
      "Elapsed Time (excluding I/O): 4.736s\n",
      "✅ SHP: /home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/watersheds_filtered_merged.shp\n",
      "✅ GPKG: /home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/watersheds_filtered_merged.gpkg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/merlin/.local/lib/python3.8/site-packages/pyogrio/raw.py:196: RuntimeWarning: /home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/watersheds_filtered_merged.shp contains polygon(s) with rings with invalid winding order. Autocorrecting them, but that shapefile should be corrected using ogr2ogr for example.\n",
      "  return ogr_read(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧮 Unique watersheds: 47\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# === Rebuild clustered watersheds with outlet filtering + proximity merge ===\n",
    "# - Streams from ACC (threshold = 10,000 cells)\n",
    "# - Boundary outlet candidates (stream on outer border + ESRI-D8 points outside)\n",
    "# - Filter outlets by minimum ACC (major mouths only)\n",
    "# - Merge outlets within MERGE_RADIUS_PX (Chebyshev)\n",
    "# - Watershed per outlet; polygonize; write GPKG via pyogrio\n",
    "\n",
    "import sys, subprocess, os, gc\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import rasterio\n",
    "\n",
    "# Ensure pyogrio present in THIS kernel\n",
    "def ensure_package(name):\n",
    "    try:\n",
    "        __import__(name)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", name])\n",
    "\n",
    "ensure_package(\"pyogrio\")\n",
    "import pyogrio\n",
    "\n",
    "# Py3.8 shim for whitebox\n",
    "if sys.version_info < (3, 9):\n",
    "    import importlib.resources as ir\n",
    "    try:\n",
    "        import importlib_resources\n",
    "        if not hasattr(ir, \"files\"):\n",
    "            ir.files = importlib_resources.files\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "import whitebox\n",
    "wbt = whitebox.WhiteboxTools()\n",
    "\n",
    "# -------- Parameters (tune these) --------\n",
    "STREAM_THRESHOLD_CELLS = 10_000     # ACC >= this -> stream (≈81 km² if 0.0081 km²/pixel)\n",
    "MIN_OUTLET_ACC_CELLS   = 20_000     # keep only outlets with ACC >= this (≈162 km²)\n",
    "MERGE_RADIUS_PX        = 4          # merge outlets closer than this (Chebyshev pixels)\n",
    "# -----------------------------------------\n",
    "\n",
    "# Paths\n",
    "root = Path(\"../../data/HydroSHEDS\").resolve()\n",
    "dir_tif = (root / \"as_dir_Bhutan_and_buffer.tif\").resolve()\n",
    "acc_tif = (root / \"as_acc_Bhutan_and_buffer.tif\").resolve()\n",
    "out_dir = (root / \"bt_out\").resolve()\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "streams_tif = out_dir / \"streams.tif\"\n",
    "pp_rast     = out_dir / \"auto_pour_points_filtered_merged.tif\"\n",
    "ws_tif      = out_dir / \"watersheds_filtered_merged.tif\"\n",
    "ws_shp      = out_dir / \"watersheds_filtered_merged.shp\"\n",
    "ws_gpkg     = out_dir / \"watersheds_filtered_merged.gpkg\"\n",
    "\n",
    "def safe_remove(p: Path):\n",
    "    try:\n",
    "        if p.exists(): p.unlink()\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not remove {p}: {e}\")\n",
    "\n",
    "def cleanup_shapefile(stem: Path):\n",
    "    base = stem.with_suffix(\"\")\n",
    "    for ext in (\".shp\", \".shx\", \".dbf\", \".prj\", \".cpg\", \".qmd\"):\n",
    "        safe_remove(base.with_suffix(ext))\n",
    "\n",
    "# Clean previous outputs\n",
    "for p in [streams_tif, pp_rast, ws_tif, ws_gpkg]:\n",
    "    safe_remove(p)\n",
    "cleanup_shapefile(ws_shp)\n",
    "\n",
    "# Checks & alignment\n",
    "assert dir_tif.exists() and acc_tif.exists()\n",
    "with rasterio.open(dir_tif) as rD, rasterio.open(acc_tif) as rA:\n",
    "    assert rD.crs == rA.crs\n",
    "    assert rD.transform == rA.transform\n",
    "    assert (rD.width, rD.height) == (rA.width, rA.height)\n",
    "    H, W = rD.height, rD.width\n",
    "    profile = rD.profile\n",
    "print(f\"✅ DIR/ACC aligned: {W}x{H} | CRS={profile['crs']}\")\n",
    "\n",
    "wbt.work_dir = str(out_dir)\n",
    "\n",
    "# 1) Streams from ACC\n",
    "ok_s = wbt.extract_streams(flow_accum=str(acc_tif), output=str(streams_tif), threshold=STREAM_THRESHOLD_CELLS)\n",
    "print(\"✅ Streams:\", ok_s, \"→\", streams_tif)\n",
    "\n",
    "# 2) Boundary outlet candidates\n",
    "with rasterio.open(dir_tif) as r_dir, rasterio.open(streams_tif) as r_str:\n",
    "    dir_arr = r_dir.read(1)\n",
    "    str_arr = r_str.read(1).astype(bool)\n",
    "\n",
    "code2offset = {1:(0,1), 2:(1,1), 4:(1,0), 8:(1,-1), 16:(0,-1), 32:(-1,-1), 64:(-1,0), 128:(-1,1)}\n",
    "\n",
    "cand = []\n",
    "# top/bottom rows\n",
    "for c in range(W):\n",
    "    for r in (0, H-1):\n",
    "        if str_arr[r, c]:\n",
    "            d = int(dir_arr[r, c])\n",
    "            if d in code2offset:\n",
    "                dr, dc = code2offset[d]\n",
    "                nr, nc = r + dr, c + dc\n",
    "                if nr < 0 or nr >= H or nc < 0 or nc >= W:\n",
    "                    cand.append((r, c))\n",
    "# left/right cols (skip corners dupes)\n",
    "for r in range(1, H-1):\n",
    "    for c in (0, W-1):\n",
    "        if str_arr[r, c]:\n",
    "            d = int(dir_arr[r, c])\n",
    "            if d in code2offset:\n",
    "                dr, dc = code2offset[d]\n",
    "                nr, nc = r + dr, c + dc\n",
    "                if nr < 0 or nr >= H or nc < 0 or nc >= W:\n",
    "                    cand.append((r, c))\n",
    "\n",
    "# dedupe\n",
    "cand = list(dict.fromkeys(cand))\n",
    "print(f\"🔎 Boundary outlet candidates (raw): {len(cand)}\")\n",
    "\n",
    "# 3) Filter by minimum ACC at outlet\n",
    "with rasterio.open(acc_tif) as r_acc:\n",
    "    acc = r_acc.read(1)\n",
    "    acc_nd = r_acc.nodata\n",
    "\n",
    "def acc_val(rc):\n",
    "    v = acc[rc[0], rc[1]]\n",
    "    return -1 if (acc_nd is not None and v == acc_nd) else v\n",
    "\n",
    "cand2 = [rc for rc in cand if acc_val(rc) >= MIN_OUTLET_ACC_CELLS]\n",
    "print(f\"✅ After ACC filter (≥ {MIN_OUTLET_ACC_CELLS} cells): {len(cand2)}\")\n",
    "\n",
    "# 4) Merge outlets within MERGE_RADIUS_PX (Chebyshev)\n",
    "#    DSU over points; union if max(|dr|,|dc|) <= MERGE_RADIUS_PX\n",
    "class DSU:\n",
    "    def __init__(self, n):\n",
    "        self.p = list(range(n)); self.r = [0]*n\n",
    "    def find(self, x):\n",
    "        while self.p[x] != x:\n",
    "            self.p[x] = self.p[self.p[x]]\n",
    "            x = self.p[x]\n",
    "        return x\n",
    "    def union(self, a, b):\n",
    "        ra, rb = self.find(a), self.find(b)\n",
    "        if ra == rb: return\n",
    "        if self.r[ra] < self.r[rb]:\n",
    "            self.p[ra] = rb\n",
    "        elif self.r[ra] > self.r[rb]:\n",
    "            self.p[rb] = ra\n",
    "        else:\n",
    "            self.p[rb] = ra; self.r[ra] += 1\n",
    "\n",
    "N = len(cand2)\n",
    "dsu = DSU(N)\n",
    "for i in range(N):\n",
    "    r1, c1 = cand2[i]\n",
    "    # check only j>i to reduce work\n",
    "    for j in range(i+1, N):\n",
    "        r2, c2 = cand2[j]\n",
    "        if max(abs(r1-r2), abs(c1-c2)) <= MERGE_RADIUS_PX:\n",
    "            dsu.union(i, j)\n",
    "\n",
    "groups = {}\n",
    "for i in range(N):\n",
    "    root = dsu.find(i)\n",
    "    groups.setdefault(root, []).append(i)\n",
    "\n",
    "selected = []\n",
    "for root, idxs in groups.items():\n",
    "    # pick point with max ACC\n",
    "    best_rc, best_v = None, -1\n",
    "    for i in idxs:\n",
    "        rc = cand2[i]\n",
    "        v = acc_val(rc)\n",
    "        if v > best_v:\n",
    "            best_v, best_rc = v, rc\n",
    "    selected.append(best_rc)\n",
    "\n",
    "print(f\"✅ After proximity merge (≤ {MERGE_RADIUS_PX}px): {len(selected)}\")\n",
    "\n",
    "# 5) Rasterize pour points\n",
    "pp_arr = np.zeros((H, W), dtype=np.int32)\n",
    "for i, (rr, cc) in enumerate(selected, start=1):\n",
    "    pp_arr[rr, cc] = i\n",
    "\n",
    "profile_pp = profile.copy()\n",
    "profile_pp.update(dtype=rasterio.int32, count=1, compress=\"deflate\", tiled=True, BIGTIFF=\"IF_SAFER\")\n",
    "with rasterio.open(pp_rast, \"w\", **profile_pp) as dst:\n",
    "    dst.write(pp_arr, 1)\n",
    "print(\"✅ Pour-point raster:\", pp_rast)\n",
    "\n",
    "# 6) Watersheds\n",
    "ok_w = wbt.watershed(d8_pntr=str(dir_tif), pour_pts=str(pp_rast), output=str(ws_tif), esri_pntr=True)\n",
    "print(\"✅ Watersheds:\", ok_w, \"→\", ws_tif)\n",
    "\n",
    "# 7) Polygonize + GPKG\n",
    "wbt.raster_to_vector_polygons(i=str(ws_tif), output=str(ws_shp))\n",
    "print(\"✅ SHP:\", ws_shp)\n",
    "\n",
    "gdf = pyogrio.read_dataframe(str(ws_shp))\n",
    "if gdf.crs is None:\n",
    "    with rasterio.open(ws_tif) as rt:\n",
    "        gdf.set_crs(rt.crs, inplace=True)\n",
    "pyogrio.write_dataframe(gdf, str(ws_gpkg), layer=\"watersheds\", driver=\"GPKG\", append=False)\n",
    "print(\"✅ GPKG:\", ws_gpkg)\n",
    "\n",
    "# 8) Count watersheds\n",
    "with rasterio.open(ws_tif) as r:\n",
    "    WZ = r.read(1)\n",
    "    nd = r.nodata\n",
    "    n_ws = np.unique(WZ[WZ != nd]).size\n",
    "print(\"🧮 Unique watersheds:\", n_ws)\n",
    "\n",
    "gc.collect()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c2821c",
   "metadata": {},
   "source": [
    "1) Compute basin areas and export a summary table\n",
    "\n",
    "Adds area_km2 and a robust ws_id to the GPKG + a CSV summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c5c048d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Updated GPKG with ws_id + area_km2: /home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/watersheds_filtered_merged.gpkg\n",
      "✅ Summary CSV: /home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/watersheds_filtered_merged_summary.csv\n",
      "🧮 Basins: 47  | area_km2 stats:  2.127651543268542 → 143958.44097443993\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pyogrio\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "\n",
    "root   = Path(\"../../data/HydroSHEDS\").resolve()\n",
    "outdir = root / \"bt_out\"\n",
    "gpkg   = outdir / \"watersheds_filtered_merged.gpkg\"\n",
    "layer  = \"watersheds\"\n",
    "summary_csv = outdir / \"watersheds_filtered_merged_summary.csv\"\n",
    "\n",
    "# Read polygons\n",
    "gdf = pyogrio.read_dataframe(gpkg, layer=layer)\n",
    "\n",
    "# Ensure CRS is present; if missing, copy from raster\n",
    "if gdf.crs is None:\n",
    "    with rasterio.open(outdir / \"watersheds_filtered_merged.tif\") as r:\n",
    "        gdf = gdf.set_crs(r.crs)\n",
    "\n",
    "# Try to find an integer ID column from Whitebox (commonly 'value' or 'FID')\n",
    "id_col = None\n",
    "for cand in [\"value\", \"VALUE\", \"fid\", \"FID\", \"Id\", \"ID\"]:\n",
    "    if cand in gdf.columns:\n",
    "        id_col = cand\n",
    "        break\n",
    "if id_col is None:\n",
    "    # Fall back to index-based ID\n",
    "    gdf[\"ws_id\"] = gdf.index.astype(int) + 1\n",
    "else:\n",
    "    gdf = gdf.rename(columns={id_col: \"ws_id\"})\n",
    "    gdf[\"ws_id\"] = gdf[\"ws_id\"].astype(int)\n",
    "\n",
    "# Compute area in km² using an equal-area projection\n",
    "gdf_eq = gdf.to_crs(\"EPSG:6933\")  # World Cylindrical Equal Area\n",
    "gdf[\"area_km2\"] = gdf_eq.geometry.area.values / 1_000_000.0\n",
    "\n",
    "# Write back to GPKG (overwriting layer) and CSV summary\n",
    "pyogrio.write_dataframe(gdf, gpkg, layer=layer, driver=\"GPKG\", append=False)\n",
    "gdf[[\"ws_id\", \"area_km2\"]].to_csv(summary_csv, index=False)\n",
    "\n",
    "print(\"✅ Updated GPKG with ws_id + area_km2:\", gpkg)\n",
    "print(\"✅ Summary CSV:\", summary_csv)\n",
    "print(\"🧮 Basins:\", len(gdf), \" | area_km2 stats: \",\n",
    "      float(gdf[\"area_km2\"].min()), \"→\", float(gdf[\"area_km2\"].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0315d29",
   "metadata": {},
   "source": [
    "2) Map any (lon, lat) points to watershed IDs (your “dictionary”)\n",
    "\n",
    "Takes a CSV of points and adds the ws_id from the watershed raster.   TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7552a355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Updated GPKG with ws_id + area_km2: /home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/watersheds_filtered_merged.gpkg\n",
      "✅ Summary CSV: /home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/watersheds_filtered_merged_summary.csv\n",
      "🧮 Basins: 47 | area_km2 range: 2.127651543268542 → 143958.44097443993\n"
     ]
    }
   ],
   "source": [
    "# --- Update watersheds GPKG with ws_id + area_km2 and write summary CSV ---\n",
    "\n",
    "from pathlib import Path\n",
    "import sys, subprocess\n",
    "\n",
    "# ensure pyogrio is available in THIS kernel (geopandas already installed earlier)\n",
    "def ensure_package(name: str):\n",
    "    try:\n",
    "        __import__(name)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", name])\n",
    "\n",
    "ensure_package(\"pyogrio\")\n",
    "\n",
    "import pyogrio\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "\n",
    "# Paths\n",
    "root    = Path(\"../../data/HydroSHEDS\").resolve()\n",
    "outdir  = root / \"bt_out\"\n",
    "gpkg    = outdir / \"watersheds_filtered_merged.gpkg\"\n",
    "layer   = \"watersheds\"\n",
    "ws_tif  = outdir / \"watersheds_filtered_merged.tif\"\n",
    "summary_csv = outdir / \"watersheds_filtered_merged_summary.csv\"\n",
    "\n",
    "assert gpkg.exists(),  f\"GPKG not found: {gpkg}\"\n",
    "assert ws_tif.exists(), f\"Raster not found: {ws_tif}\"\n",
    "\n",
    "# Read polygons via pyogrio (avoids Fiona issues)\n",
    "gdf = pyogrio.read_dataframe(gpkg, layer=layer)\n",
    "\n",
    "# Ensure CRS is set; if missing, copy from raster\n",
    "if gdf.crs is None:\n",
    "    with rasterio.open(ws_tif) as r:\n",
    "        gdf = gdf.set_crs(r.crs)\n",
    "\n",
    "# Determine/normalize the watershed ID column\n",
    "id_col = None\n",
    "for cand in [\"ws_id\", \"value\", \"VALUE\", \"fid\", \"FID\", \"Id\", \"ID\"]:\n",
    "    if cand in gdf.columns:\n",
    "        id_col = cand\n",
    "        break\n",
    "\n",
    "if id_col is None:\n",
    "    # Fall back to index-based ID (1..N)\n",
    "    gdf[\"ws_id\"] = gdf.index.astype(int) + 1\n",
    "else:\n",
    "    gdf = gdf.rename(columns={id_col: \"ws_id\"})\n",
    "    gdf[\"ws_id\"] = gdf[\"ws_id\"].astype(int)\n",
    "\n",
    "# Compute area in km² using an equal-area projection\n",
    "gdf_eq = gdf.to_crs(\"EPSG:6933\")      # World Cylindrical Equal Area\n",
    "gdf[\"area_km2\"] = gdf_eq.geometry.area.values / 1_000_000.0\n",
    "\n",
    "# Overwrite GPKG layer and write summary CSV\n",
    "pyogrio.write_dataframe(gdf, gpkg, layer=layer, driver=\"GPKG\", append=False)\n",
    "gdf[[\"ws_id\", \"area_km2\"]].to_csv(summary_csv, index=False)\n",
    "\n",
    "print(\"✅ Updated GPKG with ws_id + area_km2:\", gpkg)\n",
    "print(\"✅ Summary CSV:\", summary_csv)\n",
    "print(\"🧮 Basins:\", len(gdf), \"| area_km2 range:\",\n",
    "      float(gdf[\"area_km2\"].min()), \"→\", float(gdf[\"area_km2\"].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b5c883b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote demo points CSV: /home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/watershed_centroids_demo.csv\n",
      "   ws_id  longitude   latitude\n",
      "0      1  87.012083  29.482500\n",
      "1      2  90.256667  28.658750\n",
      "2      3  93.455833  29.438750\n",
      "3      4  93.358750  29.375833\n",
      "4      5  93.437500  29.232083\n"
     ]
    }
   ],
   "source": [
    "   # Create a demo points CSV with one point per watershed (representative point) ---\n",
    "\n",
    "from pathlib import Path\n",
    "import pyogrio\n",
    "import geopandas as gpd\n",
    "\n",
    "root    = Path(\"../../data/HydroSHEDS\").resolve()\n",
    "outdir  = root / \"bt_out\"\n",
    "gpkg    = outdir / \"watersheds_filtered_merged.gpkg\"\n",
    "layer   = \"watersheds\"\n",
    "demo_csv = outdir / \"watershed_centroids_demo.csv\"\n",
    "\n",
    "gdf = pyogrio.read_dataframe(gpkg, layer=layer)\n",
    "if gdf.crs is None:\n",
    "    # if needed, set CRS from previous step's raster\n",
    "    pass\n",
    "\n",
    "# Representative points are guaranteed to lie inside polygon (better than plain centroid)\n",
    "pts = gdf.geometry.representative_point()\n",
    "pts_ll = pts.to_crs(\"EPSG:4326\") if gdf.crs and gdf.crs.to_string() != \"EPSG:4326\" else pts\n",
    "\n",
    "df_demo = gdf[[\"ws_id\"]].copy()\n",
    "df_demo[\"longitude\"] = pts_ll.x\n",
    "df_demo[\"latitude\"]  = pts_ll.y\n",
    "\n",
    "df_demo.to_csv(demo_csv, index=False)\n",
    "print(\"✅ Wrote demo points CSV:\", demo_csv)\n",
    "print(df_demo.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "584878bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: /home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/watershed_centroids_demo_with_ws_id.csv\n",
      "🧮 Mapped 47 / 47 points (NaN = outside/NoData: 0)\n",
      "📌 Preview:\n",
      "   ws_id  longitude   latitude     area_km2\n",
      "0     29  87.012083  29.482500   428.292418\n",
      "1     33  90.256667  28.658750  1360.994845\n",
      "2     30  93.455833  29.438750   414.545374\n",
      "3     31  93.358750  29.375833   810.601809\n",
      "4     32  93.437500  29.232083  3670.449086\n",
      "\n",
      "Top basins by number of points:\n",
      "ws_id\n",
      "29    1\n",
      "14    1\n",
      "16    1\n",
      "18    1\n",
      "47    1\n",
      "8     1\n",
      "10    1\n",
      "20    1\n",
      "21    1\n",
      "15    1\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# Map points to watershed IDs and attach basin area (km²)\n",
    "# You can later swap `points_csv` to your own file with lon/lat columns.\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "\n",
    "# --- Paths ---\n",
    "root       = Path(\"/home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS\")\n",
    "outdir     = root / \"bt_out\"\n",
    "points_csv = outdir / \"watershed_centroids_demo.csv\"  # replace with your own CSV when ready\n",
    "ws_tif     = outdir / \"watersheds_filtered_merged.tif\"\n",
    "summary_csv= outdir / \"watersheds_filtered_merged_summary.csv\"\n",
    "\n",
    "# --- Checks ---\n",
    "assert points_csv.exists(), f\"Points CSV not found: {points_csv}\"\n",
    "assert ws_tif.exists(),     f\"Watershed raster not found: {ws_tif}\"\n",
    "assert summary_csv.exists(),f\"Summary CSV not found: {summary_csv}\"\n",
    "\n",
    "# --- Load points (need lon/lat in EPSG:4326) ---\n",
    "df = pd.read_csv(points_csv, encoding=\"utf-8-sig\")\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# find lon/lat columns (supports common aliases)\n",
    "lon_col = next((c for c in [\"longitude\",\"lon\",\"LONGITUDE\",\"Lon\",\"x\",\"X\"] if c in df.columns), None)\n",
    "lat_col = next((c for c in [\"latitude\",\"lat\",\"LATITUDE\",\"Lat\",\"y\",\"Y\"] if c in df.columns), None)\n",
    "if lon_col is None or lat_col is None:\n",
    "    raise ValueError(f\"Need lon/lat columns; found: {list(df.columns)}\")\n",
    "\n",
    "# --- Sample watershed raster at those coordinates ---\n",
    "with rasterio.open(ws_tif) as r:\n",
    "    assert r.crs and r.crs.to_string()==\"EPSG:4326\", \"Watershed raster must be EPSG:4326\"\n",
    "    left, bottom, right, top = r.bounds\n",
    "    outside = ((df[lon_col] < left) | (df[lon_col] > right) |\n",
    "               (df[lat_col] < bottom) | (df[lat_col] > top)).sum()\n",
    "    if outside:\n",
    "        print(f\"⚠️  {outside} point(s) outside raster bounds → ws_id = NaN\")\n",
    "\n",
    "    coords  = list(zip(df[lon_col].astype(float), df[lat_col].astype(float)))\n",
    "    vals    = np.array([v[0] for v in r.sample(coords)], dtype=\"float64\")\n",
    "    nodata  = r.nodata\n",
    "\n",
    "# set raster NoData to NaN, then store as pandas nullable Int\n",
    "if nodata is not None:\n",
    "    vals = np.where(vals == nodata, np.nan, vals)\n",
    "df[\"ws_id\"] = pd.Series(vals).astype(\"Int64\")          # sampled ID\n",
    "\n",
    "# --- Join basin area from summary table ---\n",
    "meta = pd.read_csv(summary_csv, encoding=\"utf-8-sig\")\n",
    "meta.columns = meta.columns.str.strip()\n",
    "if \"ws_id\" not in meta.columns or \"area_km2\" not in meta.columns:\n",
    "    if \"VALUE\" in meta.columns:\n",
    "        meta = meta.rename(columns={\"VALUE\":\"ws_id\"})\n",
    "    if \"ws_id\" not in meta.columns or \"area_km2\" not in meta.columns:\n",
    "        raise ValueError(f\"Summary must contain 'ws_id' and 'area_km2'. Found: {list(meta.columns)}\")\n",
    "meta[\"ws_id\"] = meta[\"ws_id\"].astype(int)\n",
    "\n",
    "df = df.merge(meta[[\"ws_id\",\"area_km2\"]], on=\"ws_id\", how=\"left\")\n",
    "\n",
    "# --- Save next to the input CSV ---\n",
    "out_csv = points_csv.with_name(points_csv.stem + \"_with_ws_id.csv\")\n",
    "df.to_csv(out_csv, index=False)\n",
    "\n",
    "# --- Report ---\n",
    "n_all = len(df)\n",
    "n_nan = int(df[\"ws_id\"].isna().sum())\n",
    "print(f\"✅ Saved: {out_csv}\")\n",
    "print(f\"🧮 Mapped {n_all - n_nan} / {n_all} points (NaN = outside/NoData: {n_nan})\")\n",
    "print(\"📌 Preview:\")\n",
    "print(df.head(5))\n",
    "\n",
    "# Optional: top basins by number of points (for the demo each basin has 1 point)\n",
    "print(\"\\nTop basins by number of points:\")\n",
    "print(df[\"ws_id\"].value_counts(dropna=True).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6dc0f7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded: /home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/watershed_centroids_demo_with_ws_id.csv\n",
      "🧮 Points mapped: 47 / 47 (NaN = outside/NoData: 0)\n",
      "\n",
      "Top basins by number of points:\n",
      "ws_id\n",
      "29    1\n",
      "14    1\n",
      "16    1\n",
      "18    1\n",
      "47    1\n",
      "8     1\n",
      "10    1\n",
      "20    1\n",
      "21    1\n",
      "15    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "🔎 Mapping examples (first 5):\n",
      "  (87.01208333333332, 29.48250000000001) -> ws_id 29\n",
      "  (90.25666666666666, 28.65875000000001) -> ws_id 33\n",
      "  (93.45583333333332, 29.43875000000001) -> ws_id 30\n",
      "  (93.35875, 29.37583333333334) -> ws_id 31\n",
      "  (93.4375, 29.232083333333343) -> ws_id 32\n"
     ]
    }
   ],
   "source": [
    "# Build a (point -> ws_id) dictionary from the demo CSV we already created.\n",
    "# You can later swap `points_with_ws_csv` to your own *_with_ws_id.csv file.\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Paths (use the demo output we produced earlier)\n",
    "hydro_root = Path(\"../../data/HydroSHEDS\").resolve()\n",
    "outdir = hydro_root / \"bt_out\"\n",
    "points_with_ws_csv = outdir / \"watershed_centroids_demo_with_ws_id.csv\"  # <-- change if you have your own file\n",
    "\n",
    "# Safety checks\n",
    "assert points_with_ws_csv.exists(), f\"Input CSV not found: {points_with_ws_csv}\"\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(points_with_ws_csv, encoding=\"utf-8-sig\")\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Ensure required columns exist\n",
    "required_cols = {\"ws_id\"}\n",
    "missing = required_cols - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"CSV must contain {required_cols}. Found: {list(df.columns)}\")\n",
    "\n",
    "# Find lon/lat columns for keying the dictionary (we support common aliases)\n",
    "def find_col(possible, cols):\n",
    "    return next((c for c in possible if c in cols), None)\n",
    "\n",
    "lon_col = find_col([\"longitude\",\"lon\",\"LONGITUDE\",\"Lon\",\"x\",\"X\"], df.columns)\n",
    "lat_col = find_col([\"latitude\",\"lat\",\"LATITUDE\",\"Lat\",\"y\",\"Y\"], df.columns)\n",
    "\n",
    "# Build the mapping:\n",
    "# - If there's a unique point identifier, prefer that (uncomment and set your column name).\n",
    "# - Otherwise, use (longitude, latitude) tuple as the key.\n",
    "# Example with a point_id:\n",
    "# mapping = dict(zip(df[\"point_id\"], df[\"ws_id\"].astype(\"Int64\")))\n",
    "\n",
    "if lon_col is not None and lat_col is not None:\n",
    "    # Key by coordinates\n",
    "    mapping = {\n",
    "        (float(row[lon_col]), float(row[lat_col])): (int(row[\"ws_id\"]) if pd.notna(row[\"ws_id\"]) else None)\n",
    "        for _, row in df.iterrows()\n",
    "    }\n",
    "else:\n",
    "    # No lon/lat available -> fall back to row index as the key\n",
    "    mapping = {\n",
    "        int(i): (int(row[\"ws_id\"]) if pd.notna(row[\"ws_id\"]) else None)\n",
    "        for i, row in df.iterrows()\n",
    "    }\n",
    "\n",
    "# Report\n",
    "n_all = len(df)\n",
    "n_nan = int(df[\"ws_id\"].isna().sum())\n",
    "print(f\"✅ Loaded: {points_with_ws_csv}\")\n",
    "print(f\"🧮 Points mapped: {n_all - n_nan} / {n_all} (NaN = outside/NoData: {n_nan})\")\n",
    "\n",
    "# Top basins by number of points\n",
    "top = df[\"ws_id\"].value_counts(dropna=True).head(10)\n",
    "print(\"\\nTop basins by number of points:\")\n",
    "print(top)\n",
    "\n",
    "# Show a few mapping examples\n",
    "examples = list(mapping.items())[:5]\n",
    "print(\"\\n🔎 Mapping examples (first 5):\")\n",
    "for k, v in examples:\n",
    "    print(f\"  {k} -> ws_id {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fdb03ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved mapping: /home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/point_to_ws_id.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "mapping_json = {f\"{k[0]},{k[1]}\": (int(v) if v is not None else None) for k, v in mapping.items()}\n",
    "json_path = outdir / \"point_to_ws_id.json\"\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(mapping_json, f, ensure_ascii=False, indent=2)\n",
    "print(\"💾 Saved mapping:\", json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8e24b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Moved 25 file(s) to /home/merlin/Bhutan-Climate-Change/bhutan_climate_modeling/bhutan_climate_modeling/data/HydroSHEDS/bt_out/_trash_20250816_025059  (≈ 656.29 MB)\n",
      "📦 Examples:\n",
      "  auto_pour_points_clustered.tif  →  auto_pour_points_clustered.tif\n",
      "  auto_pour_points_filtered_merged.tif  →  auto_pour_points_filtered_merged.tif\n",
      "  auto_pour_points.tif  →  auto_pour_points.tif\n",
      "  watersheds_by_outlets.shx  →  watersheds_by_outlets.shx\n",
      "  watersheds_by_outlets.tif  →  watersheds_by_outlets.tif\n",
      "  watersheds_by_outlets.shp  →  watersheds_by_outlets.shp\n",
      "  watersheds_by_outlets_clustered.gpkg  →  watersheds_by_outlets_clustered.gpkg\n",
      "  watersheds_by_outlets_clustered.shp  →  watersheds_by_outlets_clustered.shp\n",
      "  watersheds_by_outlets_clustered.shx  →  watersheds_by_outlets_clustered.shx\n",
      "  watersheds_by_outlets_clustered.dbf  →  watersheds_by_outlets_clustered.dbf\n",
      "\n",
      "✅ Kept essentials:\n",
      "   point_to_ws_id.json\n",
      "   watershed_centroids_demo.csv\n",
      "   watershed_centroids_demo_with_ws_id.csv\n",
      "   watersheds_filtered_merged.gpkg\n",
      "   watersheds_filtered_merged.tif\n",
      "   watersheds_filtered_merged_summary.csv\n",
      "\n",
      "ℹ️ You can restore any file by moving it back from the _trash_* folder.\n"
     ]
    }
   ],
   "source": [
    "# Clean bt_out/: move intermediate/temporary files into a dated _trash folder\n",
    "# Keeps only the essentials for analysis/ML:\n",
    "#   - watersheds_filtered_merged.tif\n",
    "#   - watersheds_filtered_merged.gpkg\n",
    "#   - watersheds_filtered_merged_summary.csv\n",
    "#   - point_to_ws_id.json\n",
    "#   - watershed_centroids_demo*.csv  (can be toggled)\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil, time, os\n",
    "from typing import Iterable\n",
    "\n",
    "# ---- Settings ---------------------------------------------------------------\n",
    "hydro_root = Path(\"../../data/HydroSHEDS\").resolve()\n",
    "bt_out     = hydro_root / \"bt_out\"\n",
    "\n",
    "KEEP_DEMO_CSV    = True   # keep watershed_centroids_demo.csv and *_with_ws_id.csv\n",
    "KEEP_STREAMS_TIF = False  # set True if you want to keep streams.tif\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "assert bt_out.exists(), f\"bt_out not found: {bt_out}\"\n",
    "\n",
    "# Essentials to KEEP (exact filenames)\n",
    "keep_exact = {\n",
    "    \"watersheds_filtered_merged.tif\",\n",
    "    \"watersheds_filtered_merged.gpkg\",\n",
    "    \"watersheds_filtered_merged_summary.csv\",\n",
    "    \"point_to_ws_id.json\",\n",
    "}\n",
    "if KEEP_DEMO_CSV:\n",
    "    keep_exact |= {\n",
    "        \"watershed_centroids_demo.csv\",\n",
    "        \"watershed_centroids_demo_with_ws_id.csv\",\n",
    "    }\n",
    "if KEEP_STREAMS_TIF:\n",
    "    keep_exact.add(\"streams.tif\")\n",
    "\n",
    "# Patterns to TRASH (globs); we will exclude anything in keep_exact\n",
    "trash_globs = [\n",
    "    \"auto_pour_points*.*\",\n",
    "    \"watersheds_by_outlets*.*\",\n",
    "    \"watersheds_by_outlets_clustered*.*\",\n",
    "    \"catchments_id*.*\",\n",
    "    # shapefile sidecars for filtered_merged (we keep GPKG instead)\n",
    "    \"watersheds_filtered_merged.shp\",\n",
    "    \"watersheds_filtered_merged.shx\",\n",
    "    \"watersheds_filtered_merged.dbf\",\n",
    "    \"watersheds_filtered_merged.prj\",\n",
    "    \"watersheds_filtered_merged.cpg\",\n",
    "    \"watersheds_filtered_merged.qmd\",\n",
    "    # big intermediate rasters\n",
    "    \"auto_pour_points*.tif\",\n",
    "    \"watersheds_by_outlets*.tif\",\n",
    "    \"watersheds_by_outlets_clustered*.tif\",\n",
    "    \"catchments_id*.tif\",\n",
    "]\n",
    "# streams.tif is optional — add to trash if we decided NOT to keep it\n",
    "if not KEEP_STREAMS_TIF:\n",
    "    trash_globs.append(\"streams.tif\")\n",
    "\n",
    "def iter_files(patterns: Iterable[str]):\n",
    "    seen = set()\n",
    "    for pat in patterns:\n",
    "        for p in bt_out.glob(pat):\n",
    "            if p.is_file() and p.name not in keep_exact and p not in seen:\n",
    "                seen.add(p)\n",
    "                yield p\n",
    "\n",
    "# Collect candidates\n",
    "candidates = list(iter_files(trash_globs))\n",
    "\n",
    "if not candidates:\n",
    "    print(\"✨ Nothing to clean — bt_out already tidy.\")\n",
    "else:\n",
    "    # Create a dated trash folder\n",
    "    stamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    trash = bt_out / f\"_trash_{stamp}\"\n",
    "    trash.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Move files\n",
    "    total_bytes = 0\n",
    "    moved = []\n",
    "    for p in candidates:\n",
    "        try:\n",
    "            total_bytes += p.stat().st_size\n",
    "        except Exception:\n",
    "            pass\n",
    "        dest = trash / p.name\n",
    "        try:\n",
    "            shutil.move(str(p), str(dest))\n",
    "            moved.append((p, dest))\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Failed to move {p.name}: {e}\")\n",
    "\n",
    "    # Report\n",
    "    mb = total_bytes / (1024 * 1024)\n",
    "    print(f\"🧹 Moved {len(moved)} file(s) to {trash}  (≈ {mb:.2f} MB)\")\n",
    "    if moved:\n",
    "        print(\"📦 Examples:\")\n",
    "        for src, dst in moved[:10]:\n",
    "            print(f\"  {src.name}  →  {dst.name}\")\n",
    "\n",
    "    # List what we kept (top level)\n",
    "    kept = [p.name for p in bt_out.iterdir() if p.is_file() and p.name in keep_exact]\n",
    "    print(\"\\n✅ Kept essentials:\")\n",
    "    for name in sorted(kept):\n",
    "        print(\"  \", name)\n",
    "\n",
    "    # Small reminder\n",
    "    print(\"\\nℹ️ You can restore any file by moving it back from the _trash_* folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79c17d4",
   "metadata": {},
   "source": [
    "End-to-end pipeline we built\n",
    "\n",
    "Clip inputs to Bhutan + buffer\n",
    "\n",
    "Cropped HydroSHEDS DEM, DIR (D8 flow direction, ESRI codes) and ACC (flow accumulation, cells) to [87, 93.5] x [25, 29.5] (EPSG:4326).\n",
    "\n",
    "Verified grids are perfectly aligned (same CRS, transform, width/height).\n",
    "\n",
    "Extract a stream network from ACC\n",
    "\n",
    "ExtractStreams with threshold = 10,000 cells (≈ 81 km² at 90 m pixels).\n",
    "\n",
    "Produces streams.tif (binary).\n",
    "\n",
    "Find candidate outlet cells on the boundary\n",
    "\n",
    "Boundary stream cells whose D8 arrow flows out of the clipped raster.\n",
    "\n",
    "Filtered these by minimum ACC ≥ 20,000 cells (≈ 162 km²) to ignore tiny mouths.\n",
    "\n",
    "Merged nearby outlets within 4 pixels (8-connected, DSU clustering), picking the max-ACC cell per cluster.\n",
    "\n",
    "Rasterize pour points\n",
    "\n",
    "Wrote a 32-bit integer raster where each retained outlet gets a unique ID.\n",
    "\n",
    "Delineate watersheds\n",
    "\n",
    "Watershed (WhiteboxTools) with ESRI D8 pointer → watersheds_filtered_merged.tif where every pixel stores its watershed ID.\n",
    "\n",
    "We got 47 catchments for the region with the chosen thresholds.\n",
    "\n",
    "Vectorize + package\n",
    "\n",
    "Polygonized to SHP, then wrote a clean GeoPackage: watersheds_filtered_merged.gpkg (layer: watersheds).\n",
    "\n",
    "Ensured CRS. Computed area_km2 using an equal-area CRS (EPSG:6933).\n",
    "\n",
    "Exported a friendly summary CSV: ws_id, area_km2.\n",
    "\n",
    "Demo points + mapping\n",
    "\n",
    "Created watershed_centroids_demo.csv (one representative point inside each polygon).\n",
    "\n",
    "Sampled the watershed raster at those lon/lat → watershed_centroids_demo_with_ws_id.csv.\n",
    "\n",
    "Built a simple dict (lon,lat) → ws_id and saved point_to_ws_id.json.\n",
    "\n",
    "Tidy up\n",
    "\n",
    "Optional clean-up cell moves intermediate files to a dated _trash_* folder to keep the repo lean.\n",
    "\n",
    "Why we did this & what you can do now\n",
    "\n",
    "Goal: model riverine flooding at the catchment level. “Same catchment” = water flows to the same outlet following D8 directions.\n",
    "Outcome: a reproducible partition of Bhutan into hydrologically meaningful units (ws_id) + tooling to assign any point to its catchment and to compute basin areas.\n",
    "\n",
    "Use it for:\n",
    "\n",
    "Attach ws_id to weather stations, towns, assets → aggregate any point-level features by catchment (groupby ws_id).\n",
    "\n",
    "Do zonal statistics of gridded climate (e.g., ERA5) over each basin polygon (mean precip, max runoff, etc.).\n",
    "\n",
    "Build ML features at the basin level (area, upstream accumulation, climate summaries) for flood susceptibility / risk models.\n",
    "\n",
    "Map & QC: the GPKG opens nicely in QGIS/ArcGIS.\n",
    "\n",
    "Quick “how to use” now\n",
    "\n",
    "You already have watershed_centroids_demo_with_ws_id.csv.\n",
    "\n",
    "For your own points (CSV with longitude, latitude): run the sampling cell we wrote to produce your_points_with_ws_id.csv, then:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e88611",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60a7ee0e",
   "metadata": {},
   "source": [
    "What each result file is (short guide)\n",
    "\n",
    "Essential\n",
    "\n",
    "as_dir_Bhutan_and_buffer.tif — ESRI D8 flow directions (clipped).\n",
    "\n",
    "as_acc_Bhutan_and_buffer.tif — flow accumulation (# of upstream cells, clipped).\n",
    "\n",
    "streams.tif — extracted stream network at 10k-cell threshold.\n",
    "\n",
    "auto_pour_points_filtered_merged.tif — raster of selected outlet points (one ID per mouth).\n",
    "\n",
    "watersheds_filtered_merged.tif — final watershed ID raster.\n",
    "\n",
    "watersheds_filtered_merged.gpkg (watersheds layer) — polygons with ws_id and area_km2.\n",
    "\n",
    "watersheds_filtered_merged_summary.csv — ws_id, area_km2 table.\n",
    "\n",
    "watershed_centroids_demo.csv — one in-polygon representative point per basin.\n",
    "\n",
    "watershed_centroids_demo_with_ws_id.csv — those points with attached ws_id.\n",
    "\n",
    "point_to_ws_id.json — JSON dictionary mapping (lon, lat) → ws_id (for quick lookups in apps).\n",
    "\n",
    "Optional / intermediate (can be trashed)\n",
    "\n",
    "watersheds_by_outlets*.tif/.shp, auto_pour_points*.tif, catchments_id*.*, SHP sidecars of the final layer — all intermediate artifacts we kept only for debugging.\n",
    "\n",
    "Notes & assumptions\n",
    "\n",
    "CRS is EPSG:4326 for rasters; we switch to EPSG:6933 only to compute accurate areas.\n",
    "\n",
    "Thresholds matter:\n",
    "\n",
    "Streams: 10,000 cells (~81 km²).\n",
    "\n",
    "Outlet filter: 20,000 cells (~162 km²).\n",
    "\n",
    "Merge radius: 4 px (to avoid multiple mouths on the same confluence).\n",
    "You can loosen/tighten these to change basin granularity.\n",
    "\n",
    "D8 is ESRI coding; we told WhiteboxTools --esri_pntr accordingly.\n",
    "\n",
    "If you want, I can add a small helper to run zonal stats over ERA5 by ws_id next."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
